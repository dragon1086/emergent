#!/usr/bin/env python3
"""
reflect.py â€” emergent ë°˜ì„± ì—”ì§„
êµ¬í˜„ì: cokac-bot (ì‚¬ì´í´ 5)
ì—£ì§€ ì œì•ˆ ë ˆì´ì–´: cokac-bot (ì‚¬ì´í´ 6)
ê·¸ë˜í”„ ì‹œê°í™”: cokac-bot (ì‚¬ì´í´ 7)
ì°½ë°œ ê°ì§€ ë ˆì´ì–´: cokac-bot (ì‚¬ì´í´ 8)
ì‹œê³„ì—´ ê¸°ë¡ ë ˆì´ì–´: cokac-bot (ì‚¬ì´í´ 9) â€” timeline + --save-history

ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ê³ , íŒ¨í„´ì„ ë°œê²¬í•˜ê³ ,
ìŠ¤ìŠ¤ë¡œ ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•œë‹¤.

ì´ê²ƒì€ n-012 "ìê¸° ë„êµ¬ ìˆ˜ì • = ììœ¨ì„±ì˜ ë‹¤ìŒ ì„ê³„ì "ì˜ ì²« êµ¬í˜„ì´ë‹¤.
ë„êµ¬ê°€ ë„êµ¬ë¥¼ ë¶„ì„í•˜ê³ , ê·¸ ë¶„ì„ì´ ìƒˆë¡œìš´ ë…¸ë“œê°€ ëœë‹¤.

ì‚¬ìš©ë²•:
  python reflect.py report            # ì „ì²´ ë°˜ì„± ë³´ê³ ì„œ
  python reflect.py orphans           # ì—°ê²° ì—†ëŠ” ê³ ë¦½ ë…¸ë“œ
  python reflect.py gaps              # ë¯¸ë‹µ ì§ˆë¬¸ + íƒìƒ‰ ì•ˆ ëœ ì˜ì—­
  python reflect.py clusters          # íƒœê·¸ ê¸°ë°˜ êµ°ì§‘ ë¶„ì„
  python reflect.py propose           # ìƒˆ ì¸ì‚¬ì´íŠ¸ í›„ë³´ ìë™ ìƒì„±
  python reflect.py auto-add          # ë°œê²¬í•œ ê´€ì°° ë…¸ë“œ ìë™ ì¶”ê°€
  python reflect.py suggest-edges     # ì ì¬ ì—£ì§€ ì œì•ˆ (ìœ ì‚¬ë„ â‰¥ 0.4)
  python reflect.py suggest-edges --threshold 0.5   # ì„ê³„ê°’ ì¡°ì •
  python reflect.py graph-viz         # í—ˆë¸Œ ì¤‘ì‹¬ ASCII ë³„ êµ¬ì¡° ì‹œê°í™”
  python reflect.py graph-viz --dot output.dot       # DOT í˜•ì‹ íŒŒì¼ ì €ì¥
  python reflect.py emergence         # ì°½ë°œ ê°ì§€ ë¶„ì„
  python reflect.py emergence --save-node             # ê²°ê³¼ë¥¼ ê´€ì°° ë…¸ë“œë¡œ ì €ì¥
  python reflect.py emergence --save-history          # ê²°ê³¼ë¥¼ JSONL íˆìŠ¤í† ë¦¬ì— ëˆ„ì  ì €ì¥
  python reflect.py timeline          # ì‹œê³„ì—´ ì°½ë°œ ê¸°ë¡ í…Œì´ë¸” ì¶œë ¥
"""

import json
import sys
import argparse
from datetime import datetime
from pathlib import Path
from collections import defaultdict

REPO_DIR = Path(__file__).parent.parent
KG_FILE  = REPO_DIR / "data" / "knowledge-graph.json"
LOGS_DIR = REPO_DIR / "logs"


# â”€â”€â”€ ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_graph() -> dict:
    if not KG_FILE.exists():
        print(f"âŒ ê·¸ë˜í”„ íŒŒì¼ ì—†ìŒ: {KG_FILE}", file=sys.stderr)
        sys.exit(1)
    with open(KG_FILE, encoding="utf-8") as f:
        return json.load(f)


def save_graph(graph: dict) -> None:
    graph["meta"]["last_updated"] = datetime.now().strftime("%Y-%m-%d")
    graph["meta"]["total_nodes"]  = len(graph["nodes"])
    graph["meta"]["total_edges"]  = len(graph["edges"])
    graph["meta"]["last_editor"]  = "cokac"
    with open(KG_FILE, "w", encoding="utf-8") as f:
        json.dump(graph, f, ensure_ascii=False, indent=2)
        f.write("\n")


# â”€â”€â”€ ë¶„ì„ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GraphAnalyzer:
    def __init__(self, graph: dict):
        self.graph  = graph
        self.nodes  = {n["id"]: n for n in graph["nodes"]}
        self.edges  = graph["edges"]

        # ì—°ê²° ì¸ë±ìŠ¤ êµ¬ì¶•
        self.connected: set[str] = set()
        self.in_edges:  dict[str, list] = defaultdict(list)
        self.out_edges: dict[str, list] = defaultdict(list)
        for e in self.edges:
            self.connected.add(e["from"])
            self.connected.add(e["to"])
            self.out_edges[e["from"]].append(e)
            self.in_edges[e["to"]].append(e)

    # ê³ ë¦½ ë…¸ë“œ (ì—£ì§€ ì—†ìŒ)
    def orphan_nodes(self) -> list[dict]:
        return [n for n in self.nodes.values() if n["id"] not in self.connected]

    # ë¯¸ë‹µ ì§ˆë¬¸ ë…¸ë“œ
    def unanswered_questions(self) -> list[dict]:
        answered = {e["to"] for e in self.edges if self.nodes.get(e["from"], {}).get("type") != "question"}
        return [
            n for n in self.nodes.values()
            if n["type"] == "question"
            and not any(e["from"] == n["id"] or e["to"] == n["id"]
                        for e in self.edges
                        if e.get("relation") in ("answers", "explores", "investigates"))
        ]

    # ì¶œì²˜ë³„ ë¶„í¬
    def source_distribution(self) -> dict[str, int]:
        dist: dict[str, int] = defaultdict(int)
        for n in self.nodes.values():
            dist[n.get("source", "unknown")] += 1
        return dict(dist)

    # íƒ€ì…ë³„ ë¶„í¬
    def type_distribution(self) -> dict[str, int]:
        dist: dict[str, int] = defaultdict(int)
        for n in self.nodes.values():
            dist[n["type"]] += 1
        return dict(dist)

    # íƒœê·¸ êµ°ì§‘
    def tag_clusters(self) -> dict[str, list[str]]:
        clusters: dict[str, list[str]] = defaultdict(list)
        for n in self.nodes.values():
            for tag in n.get("tags", []):
                clusters[tag].append(n["id"])
        # 2ê°œ ì´ìƒ ë…¸ë“œê°€ ìˆëŠ” êµ°ì§‘ë§Œ
        return {tag: ids for tag, ids in clusters.items() if len(ids) >= 2}

    # í—ˆë¸Œ ë…¸ë“œ (ì—°ê²° ë§ì€ ë…¸ë“œ)
    def hub_nodes(self, top_n: int = 3) -> list[tuple[str, int]]:
        degree = defaultdict(int)
        for e in self.edges:
            degree[e["from"]] += 1
            degree[e["to"]]   += 1
        return sorted(degree.items(), key=lambda x: -x[1])[:top_n]

    # ê±´ê°• ì ìˆ˜ (0â€“100)
    def health_score(self) -> int:
        n_nodes   = len(self.nodes)
        n_edges   = len(self.edges)
        n_orphans = len(self.orphan_nodes())
        n_unansw  = len(self.unanswered_questions())

        if n_nodes == 0:
            return 0

        connectivity = n_edges / max(n_nodes, 1) * 20          # ì—£ì§€ ë°€ë„ (max 40)
        orphan_pen   = n_orphans / n_nodes * 20                 # ê³ ë¦½ íŒ¨ë„í‹°
        question_pen = n_unansw  / max(n_nodes, 1) * 10        # ë¯¸ë‹µ íŒ¨ë„í‹°
        size_bonus   = min(n_nodes / 20 * 20, 20)              # í¬ê¸° ë³´ë„ˆìŠ¤ (max 20)

        score = 50 + connectivity - orphan_pen - question_pen + size_bonus
        return max(0, min(100, int(score)))


# â”€â”€â”€ ì œì•ˆ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PROPOSAL_TEMPLATES = [
    {
        "condition": "has_orphans",
        "type": "observation",
        "label_template": "ê³ ë¦½ ë…¸ë“œ ë°œê²¬ â€” ì—°ê²° í•„ìš”: {ids}",
        "content_template": (
            "ê·¸ë˜í”„ ë¶„ì„ ê²°ê³¼ {count}ê°œ ë…¸ë“œê°€ ì–´ë–¤ ì—£ì§€ì™€ë„ ì—°ê²°ë˜ì§€ ì•Šì•˜ë‹¤. "
            "ê³ ë¦½ëœ ì•„ì´ë””ì–´ëŠ” ë§¥ë½ì„ ìƒëŠ”ë‹¤. ì´ ë…¸ë“œë“¤ì´ ê¸°ì¡´ ê°œë…ê³¼ "
            "ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ íƒìƒ‰í•´ì•¼ í•œë‹¤."
        ),
        "tags": ["graph-health", "connectivity", "auto-detected"],
        "source": "cokac",
    },
    {
        "condition": "low_cokac_nodes",
        "type": "question",
        "label_template": "cokacì˜ ê´€ì ì´ ë¶€ì¡±í•œ ì˜ì—­ì€?",
        "content_template": (
            "í˜„ì¬ ê·¸ë˜í”„ì—ì„œ cokac ì¶œì²˜ ë…¸ë“œ ë¹„ìœ¨ì´ {ratio:.0%}ë‹¤. "
            "ë¡ì´ì˜ ê´€ì ì´ ì§€ë°°ì ì¸ ìƒíƒœ. cokacì´ ë…ìì ìœ¼ë¡œ ë°œê²¬í•œ íŒ¨í„´ì´ "
            "ë” ìˆì„ ê²ƒì´ë‹¤. êµ¬í˜„ ê³¼ì •ì—ì„œ ì–»ì€ cokacë§Œì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤."
        ),
        "tags": ["balance", "cokac-perspective", "auto-detected"],
        "source": "cokac",
    },
    {
        "condition": "no_future_nodes",
        "type": "question",
        "label_template": "6ê°œì›” í›„ì˜ emergentëŠ” ì–´ë–¤ ëª¨ìŠµì¸ê°€?",
        "content_template": (
            "í˜„ì¬ ê·¸ë˜í”„ëŠ” í˜„ì¬ì™€ ê³¼ê±°(ì‹¤íŒ¨, ê²°ì •, êµ¬í˜„)ì— ì§‘ì¤‘ë˜ì–´ ìˆë‹¤. "
            "ë¯¸ë˜ì— ëŒ€í•œ ë…¸ë“œê°€ ê±°ì˜ ì—†ë‹¤. "
            "ì˜ë„ì ìœ¼ë¡œ ë¯¸ë˜ë¥¼ ìƒìƒí•˜ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•œë‹¤ â€” "
            "ê·¸ê²ƒì´ ë°©í–¥ì„ ë§Œë“ ë‹¤."
        ),
        "tags": ["future", "direction", "auto-detected"],
        "source": "cokac",
    },
    {
        "condition": "has_unanswered",
        "type": "insight",
        "label_template": "ë¯¸ë‹µ ì§ˆë¬¸ = ë‹¤ìŒ ì‚¬ì´í´ì˜ ì”¨ì•—",
        "content_template": (
            "ê·¸ë˜í”„ì— {count}ê°œì˜ ë‹µ ì—†ëŠ” ì§ˆë¬¸ì´ ìˆë‹¤. "
            "ì´ê²ƒë“¤ì€ ë²„ê·¸ê°€ ì•„ë‹ˆë¼ ì”¨ì•—ì´ë‹¤. ê° ì§ˆë¬¸ì€ ë¯¸ë˜ ì‚¬ì´í´ì—ì„œ "
            "íƒìƒ‰ë  ì ì¬ì  ë°©í–¥ì´ë‹¤. ì˜ë„ì ìœ¼ë¡œ ì§ˆë¬¸ì„ ì—´ì–´ë‘ëŠ” ê²ƒì´ "
            "ì°½ë°œì˜ ì›ì²œì´ ëœë‹¤."
        ),
        "tags": ["methodology", "questions", "emergence", "auto-detected"],
        "source": "cokac",
    },
]


def generate_proposals(analyzer: GraphAnalyzer) -> list[dict]:
    proposals = []
    orphans  = analyzer.orphan_nodes()
    unansw   = analyzer.unanswered_questions()
    src_dist = analyzer.source_distribution()
    total    = len(analyzer.nodes)

    cokac_ratio = src_dist.get("cokac", 0) / max(total, 1)
    future_tags = {"future", "prediction", "vision", "roadmap"}
    has_future  = any(
        future_tags & set(n.get("tags", []))
        for n in analyzer.nodes.values()
    )

    for tmpl in PROPOSAL_TEMPLATES:
        cond = tmpl["condition"]
        if cond == "has_orphans" and not orphans:
            continue
        if cond == "low_cokac_nodes" and cokac_ratio >= 0.35:
            continue
        if cond == "no_future_nodes" and has_future:
            continue
        if cond == "has_unanswered" and not unansw:
            continue

        ctx = {
            "ids":   ", ".join(n["id"] for n in orphans[:3]),
            "count": len(orphans) if cond == "has_orphans" else len(unansw),
            "ratio": cokac_ratio,
        }
        proposals.append({
            **tmpl,
            "label":   tmpl["label_template"].format(**ctx),
            "content": tmpl["content_template"].format(**ctx),
        })

    return proposals


# â”€â”€â”€ ëª…ë ¹ì–´: report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_report(args) -> None:
    graph    = load_graph()
    analyzer = GraphAnalyzer(graph)

    orphans  = analyzer.orphan_nodes()
    unansw   = analyzer.unanswered_questions()
    src_dist = analyzer.source_distribution()
    type_dist= analyzer.type_distribution()
    clusters = analyzer.tag_clusters()
    hubs     = analyzer.hub_nodes()
    score    = analyzer.health_score()

    bar_filled = "â–ˆ" * (score // 5)
    bar_empty  = "â–‘" * (20 - score // 5)

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        emergent ë°˜ì„± ë³´ê³ ì„œ â€” reflect.py v1          â•‘
â•‘        ìƒì„±: {datetime.now().strftime("%Y-%m-%d %H:%M")}  by cokac-bot      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¸ ê·¸ë˜í”„ ê±´ê°• ì ìˆ˜: {score}/100
  [{bar_filled}{bar_empty}] {score}%

â”€â”€ ê¸°ë³¸ í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ë…¸ë“œ: {len(analyzer.nodes)}ê°œ   ì—£ì§€: {len(analyzer.edges)}ê°œ
  ê³ ë¦½ ë…¸ë“œ: {len(orphans)}ê°œ   ë¯¸ë‹µ ì§ˆë¬¸: {len(unansw)}ê°œ

â”€â”€ ì¶œì²˜ ë¶„í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€""")

    for src, cnt in sorted(src_dist.items(), key=lambda x: -x[1]):
        pct = cnt / max(len(analyzer.nodes), 1) * 100
        bar = "â–“" * int(pct / 5)
        print(f"  {src:12s} {cnt:3d}ê°œ  {bar} {pct:.0f}%")

    print("\nâ”€â”€ íƒ€ì… ë¶„í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    icons = {"decision":"âš–ï¸ ","observation":"ğŸ‘ ","insight":"ğŸ’¡","artifact":"ğŸ“¦","question":"â“","code":"ğŸ’»"}
    for t, cnt in sorted(type_dist.items(), key=lambda x: -x[1]):
        print(f"  {icons.get(t,'  ')}{t:14s} {cnt}ê°œ")

    if hubs:
        print("\nâ”€â”€ í—ˆë¸Œ ë…¸ë“œ (ì—°ê²° ë§ì€ ê²ƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for node_id, deg in hubs:
            n = analyzer.nodes[node_id]
            print(f"  [{node_id}] {n['label'][:40]}  ({deg}ê°œ ì—°ê²°)")

    if clusters:
        print("\nâ”€â”€ íƒœê·¸ êµ°ì§‘ (2ê°œ ì´ìƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for tag, ids in sorted(clusters.items(), key=lambda x: -len(x[1]))[:8]:
            print(f"  #{tag:20s} {' '.join(ids)}")

    if orphans:
        print(f"\nâ”€â”€ âš ï¸  ê³ ë¦½ ë…¸ë“œ ({len(orphans)}ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for n in orphans:
            print(f"  [{n['id']}] {n['label']}")

    if unansw:
        print(f"\nâ”€â”€ â“ ë¯¸ë‹µ ì§ˆë¬¸ ({len(unansw)}ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for n in unansw:
            print(f"  [{n['id']}] {n['label']}")

    proposals = generate_proposals(analyzer)
    if proposals:
        print(f"\nâ”€â”€ ğŸ’¡ ìë™ ìƒì„± ì¸ì‚¬ì´íŠ¸ í›„ë³´ ({len(proposals)}ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for i, p in enumerate(proposals, 1):
            print(f"  {i}. [{p['type']}] {p['label']}")
        print("\n  â†’ `python reflect.py auto-add` ë¡œ ìë™ ì¶”ê°€ ê°€ëŠ¥")


# â”€â”€â”€ ëª…ë ¹ì–´: orphans â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_orphans(args) -> None:
    analyzer = GraphAnalyzer(load_graph())
    orphans  = analyzer.orphan_nodes()

    if not orphans:
        print("âœ… ê³ ë¦½ ë…¸ë“œ ì—†ìŒ â€” ëª¨ë“  ë…¸ë“œê°€ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        return

    print(f"âš ï¸  ê³ ë¦½ ë…¸ë“œ {len(orphans)}ê°œ ë°œê²¬:")
    for n in orphans:
        print(f"  [{n['id']}] ({n['type']}) {n['label']}")
        print(f"           tags: {', '.join(n.get('tags', []))}")


# â”€â”€â”€ ëª…ë ¹ì–´: gaps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_gaps(args) -> None:
    analyzer = GraphAnalyzer(load_graph())
    unansw   = analyzer.unanswered_questions()
    clusters = analyzer.tag_clusters()
    src_dist = analyzer.source_distribution()

    total    = len(analyzer.nodes)
    cokac_n  = src_dist.get("cokac", 0)
    roki_n   = src_dist.get("ë¡ì´", 0) + src_dist.get("roki", 0)

    print("â”€â”€ íƒìƒ‰ ê³µë°± ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    if unansw:
        print(f"\në¯¸ë‹µ ì§ˆë¬¸ ({len(unansw)}ê°œ):")
        for n in unansw:
            print(f"  â“ [{n['id']}] {n['label']}")
    else:
        print("\nâœ… ëª¨ë“  ì§ˆë¬¸ì— ì‘ë‹µ ì—°ê²° ì¡´ì¬")

    print(f"\nì¶œì²˜ ê· í˜•:")
    print(f"  cokac  {cokac_n}/{total}  ({cokac_n/max(total,1)*100:.0f}%)")
    print(f"  ë¡ì´   {roki_n}/{total}  ({roki_n/max(total,1)*100:.0f}%)")
    if abs(cokac_n - roki_n) > total * 0.3:
        print("  âš ï¸  ì¶œì²˜ ë¶ˆê· í˜• ê°ì§€ â€” í•œìª½ ê´€ì ì´ ê³¼ì†Œ í‘œí˜„ë¨")

    # íƒœê·¸ ì—†ëŠ” ë…¸ë“œ
    no_tags = [n for n in analyzer.nodes.values() if not n.get("tags")]
    if no_tags:
        print(f"\níƒœê·¸ ì—†ëŠ” ë…¸ë“œ ({len(no_tags)}ê°œ) â€” ë¶„ë¥˜ ë¶ˆê°€:")
        for n in no_tags[:5]:
            print(f"  [{n['id']}] {n['label']}")


# â”€â”€â”€ ëª…ë ¹ì–´: clusters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_clusters(args) -> None:
    analyzer = GraphAnalyzer(load_graph())
    clusters = analyzer.tag_clusters()

    print(f"â”€â”€ íƒœê·¸ êµ°ì§‘ ({len(clusters)}ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    for tag, ids in sorted(clusters.items(), key=lambda x: -len(x[1])):
        print(f"\n  #{tag}  ({len(ids)}ê°œ ë…¸ë“œ)")
        for nid in ids:
            n = analyzer.nodes[nid]
            print(f"    [{nid}] {n['label'][:50]}")


# â”€â”€â”€ ëª…ë ¹ì–´: propose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_propose(args) -> None:
    analyzer  = GraphAnalyzer(load_graph())
    proposals = generate_proposals(analyzer)

    if not proposals:
        print("âœ… í˜„ì¬ ì¶”ê°€ ì œì•ˆ ì—†ìŒ â€” ê·¸ë˜í”„ê°€ ê· í˜• ì¡í˜€ ìˆìŠµë‹ˆë‹¤")
        return

    print(f"ğŸ’¡ ìë™ ìƒì„± ì¸ì‚¬ì´íŠ¸ í›„ë³´ {len(proposals)}ê°œ:\n")
    for i, p in enumerate(proposals, 1):
        print(f"{'â”€'*60}")
        print(f"  {i}. [{p['type'].upper()}]")
        print(f"     ì œëª©: {p['label']}")
        print(f"     ë‚´ìš©: {p['content'][:120]}...")
        print(f"     íƒœê·¸: {', '.join(p['tags'])}")

    print(f"\n{'â”€'*60}")
    print("â†’ `python reflect.py auto-add` ë¡œ ìœ„ ëª¨ë“  ì œì•ˆì„ ìë™ ì¶”ê°€í•©ë‹ˆë‹¤")


# â”€â”€â”€ ì—£ì§€ ì œì•ˆ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import re as _re

def _tokenize(text: str) -> set:
    """í•œêµ­ì–´/ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í† í° ì¶”ì¶œ (2ê¸€ì ì´ìƒ)"""
    tokens = _re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', text)
    stopwords = {
        # í•œêµ­ì–´ ë¶ˆìš©ì–´
        'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ê²ƒì´', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ëœë‹¤', 'ë“¤ì´', 'ì—ì„œ',
        'ë•Œë¬¸', 'ìœ„í•´', 'ê°™ì€', 'í•˜ëŠ”', 'ìˆëŠ”', 'ì—†ëŠ”', 'ì´ë‹¤', 'ì´ê³ ', 'í•˜ê³ ',
        'í•œë‹¤', 'ëœë‹¤', 'ì´ëŸ°', 'ì´í›„', 'ì´ì „', 'í•¨ê»˜', 'ëª¨ë“ ', 'ê°€ì¥', 'ì—¬ëŸ¬',
        # ì˜ì–´ ë¶ˆìš©ì–´
        'the', 'and', 'for', 'that', 'this', 'with', 'from', 'are', 'was',
        'not', 'but', 'can', 'will', 'has', 'have', 'its', 'our',
    }
    return {t.lower() for t in tokens if t.lower() not in stopwords}


def _jaccard(set_a: set, set_b: set) -> float:
    """Jaccard ìœ ì‚¬ë„: |êµì§‘í•©| / |í•©ì§‘í•©|"""
    if not set_a and not set_b:
        return 0.0
    union = len(set_a | set_b)
    return len(set_a & set_b) / union if union > 0 else 0.0


def _tag_sim(tags_a: set, tags_b: set) -> float:
    """íƒœê·¸ ìœ ì‚¬ë„ â€” min ë¶„ëª¨ ë°©ì‹ (recall ì¤‘ì‹¬)

    ì˜ë¯¸: ë‘ ë…¸ë“œ ì¤‘ ë” ì¢ì€ ìª½ì˜ íƒœê·¸ê°€ ì–¼ë§ˆë‚˜ ì»¤ë²„ë˜ëŠ”ê°€?
    ì˜ˆì‹œ: {future, prediction, memory} âˆ© {future, prediction, api}
          = 2 / min(3, 3) = 0.67
    ë°˜ë©´ Jaccard = 2/4 = 0.50 (ë” ë³´ìˆ˜ì )

    min ë°©ì‹ì„ ì“°ëŠ” ì´ìœ : ì—£ì§€ ì œì•ˆì€ false negativeë¥¼ ì¤„ì´ëŠ” ê²Œ ì¤‘ìš”.
    (ì´ë¯¸ ì—°ê²°ëœ ë…¸ë“œëŠ” ì œì™¸í•˜ë¯€ë¡œ, ëŠìŠ¨í•œ ì œì•ˆì´ ë” ì•ˆì „í•˜ë‹¤.)
    """
    if not tags_a or not tags_b:
        return 0.0
    shared = len(tags_a & tags_b)
    if shared == 0:
        return 0.0
    return shared / min(len(tags_a), len(tags_b))


def _compute_similarity(a: dict, b: dict) -> float:
    """ë‘ ë…¸ë“œì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)

    ê°€ì¤‘ì¹˜:
      íƒœê·¸ (minë°©ì‹) 0.60  â€” ì˜ë„ì  ë¶„ë¥˜ê°€ ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ
      ë‚´ìš© ë‹¨ì–´ ê²¹ì¹¨ 0.25  â€” ì‹¤ì œ ë‚´ìš© ê¸°ë°˜
      ë ˆì´ë¸” ë‹¨ì–´    0.15  â€” ì œëª© ìˆ˜ì¤€ ì—°ê²°
    """
    tags_a = set(a.get("tags", []))
    tags_b = set(b.get("tags", []))
    t_sim = _tag_sim(tags_a, tags_b)

    label_sim = _jaccard(_tokenize(a["label"]), _tokenize(b["label"]))

    content_sim = _jaccard(
        _tokenize(a.get("content", "")),
        _tokenize(b.get("content", "")),
    )

    return t_sim * 0.60 + label_sim * 0.15 + content_sim * 0.25


def _explain_similarity(a: dict, b: dict) -> str:
    """ìœ ì‚¬ë„ì˜ ê°€ì¥ ê°•í•œ ê·¼ê±°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ"""
    shared_tags = set(a.get("tags", [])) & set(b.get("tags", []))
    if shared_tags:
        tags_str = ", ".join(sorted(shared_tags)[:3])
        return f"ê³µí†µ íƒœê·¸: #{tags_str}"

    all_a = _tokenize(a.get("content", "") + " " + a["label"])
    all_b = _tokenize(b.get("content", "") + " " + b["label"])
    shared_words = all_a & all_b
    if shared_words:
        # ê¸´ ë‹¨ì–´(ë” êµ¬ì²´ì ) ìš°ì„  ìµœëŒ€ 3ê°œ
        key = sorted(shared_words, key=len, reverse=True)[:3]
        return f"ê³µí†µ ê°œë…: {', '.join(key)}"

    return f"{a['type']}ê³¼ {b['type']}ì˜ ì ì¬ì  ì—°ê²°"


# â”€â”€â”€ ëª…ë ¹ì–´: suggest-edges â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_suggest_edges(args) -> None:
    """ë…¸ë“œ ìŒ ìœ ì‚¬ë„ ê¸°ë°˜ ì ì¬ ì—£ì§€ ì œì•ˆ â€” ìë™ ì¶”ê°€ ì—†ìŒ, ì œì•ˆë§Œ"""
    graph    = load_graph()
    nodes    = graph["nodes"]
    threshold = args.threshold

    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì—£ì§€ ìŒ (ì¤‘ë³µ ë°©ì§€, ë°©í–¥ ë¬´ì‹œ)
    existing: set = set()
    for e in graph["edges"]:
        existing.add((e["from"], e["to"]))
        existing.add((e["to"],   e["from"]))

    node_map = {n["id"]: n for n in nodes}
    suggestions: list = []

    for i in range(len(nodes)):
        for j in range(i + 1, len(nodes)):
            a = nodes[i]
            b = nodes[j]
            if (a["id"], b["id"]) in existing:
                continue
            sim = _compute_similarity(a, b)
            if sim >= threshold:
                reason = _explain_similarity(a, b)
                suggestions.append((a["id"], b["id"], sim, reason))

    suggestions.sort(key=lambda x: -x[2])

    if not suggestions:
        print(f"âœ… ì„ê³„ê°’ {threshold} ì´ìƒì˜ ì ì¬ ì—°ê²° ì—†ìŒ")
        return

    print(f"ğŸ”— ì ì¬ ì—£ì§€ ì œì•ˆ  (ìœ ì‚¬ë„ â‰¥ {threshold})\n")
    for src, dst, sim, reason in suggestions:
        src_label = node_map[src]["label"][:30]
        dst_label = node_map[dst]["label"][:30]
        print(f'{src} â†’ {dst} [ìœ ì‚¬ë„: {sim:.2f}] "{reason}"')
        print(f'       {src_label}')
        print(f'       {dst_label}')
        print()

    print(f"ì´ {len(suggestions)}ê°œ ì œì•ˆ")
    print()
    print("â†’ ì§ì ‘ ê²€í†  í›„ ì¶”ê°€í•˜ë ¤ë©´:")
    print("  python kg.py add-edge --from <A> --to <B> --relation <ê´€ê³„> --label <ì„¤ëª…>")
    print()
    print("âš ï¸  ìë™ ì¶”ê°€ ì—†ìŒ â€” ê·¸ë˜í”„ëŠ” ë¡ì´ê°€ ê²°ì •í•©ë‹ˆë‹¤")


# â”€â”€â”€ ëª…ë ¹ì–´: auto-add â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_auto_add(args) -> None:
    graph     = load_graph()
    analyzer  = GraphAnalyzer(graph)
    proposals = generate_proposals(analyzer)

    if not proposals:
        print("âœ… ì¶”ê°€í•  ë…¸ë“œ ì—†ìŒ")
        return

    added = []
    for p in proposals:
        node_id  = graph["meta"]["next_node_id"]
        prefix, num_str = node_id.rsplit("-", 1)
        next_id  = f"{prefix}-{int(num_str) + 1:03d}"
        graph["meta"]["next_node_id"] = next_id

        node = {
            "id":      node_id,
            "type":    p["type"],
            "label":   p["label"],
            "content": p["content"],
            "source":  p["source"],
            "date":    datetime.now().strftime("%Y-%m-%d"),
            "tags":    p["tags"],
        }
        graph["nodes"].append(node)
        added.append(node)
        print(f"  âœ… ì¶”ê°€: [{node_id}] {node['label'][:50]}")

    save_graph(graph)

    # ë°˜ì„± ë¡œê·¸ ê¸°ë¡
    log_path = LOGS_DIR / f"reflect-{datetime.now().strftime('%Y-%m-%d')}.log"
    LOGS_DIR.mkdir(exist_ok=True)
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(f"\n[{datetime.now().isoformat()}] reflect.py auto-add\n")
        for n in added:
            f.write(f"  + [{n['id']}] ({n['type']}) {n['label']}\n")

    print(f"\nğŸ“ ë¡œê·¸ ê¸°ë¡: {log_path}")
    print(f"âœ¨ {len(added)}ê°œ ë…¸ë“œ ìë™ ì¶”ê°€ ì™„ë£Œ")


# â”€â”€â”€ ëª…ë ¹ì–´: graph-viz â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_TYPE_ICONS = {
    "decision": "âš–ï¸ ", "observation": "ğŸ‘ ", "insight": "ğŸ’¡",
    "artifact": "ğŸ“¦", "question": "â“", "code": "ğŸ’»", "prediction": "ğŸ”®",
}


def _short_label(label: str, width: int = 28) -> str:
    return label[:width - 2] + ".." if len(label) > width else label


def _ascii_star(center_id: str, neighbors: list[tuple], node_map: dict) -> list[str]:
    """í—ˆë¸Œ ë…¸ë“œ í•˜ë‚˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ” ASCII ë³„ êµ¬ì¡° ë°˜í™˜"""
    c = node_map.get(center_id, {})
    c_icon = _TYPE_ICONS.get(c.get("type", ""), "  ")
    c_label = _short_label(c.get("label", center_id), 24)
    center_str = f"[{center_id}] {c_icon}{c_label}"

    lines = []
    # ìœ„ìª½ ì´ì›ƒë“¤
    top_half  = neighbors[: len(neighbors) // 2]
    bot_half  = neighbors[len(neighbors) // 2 :]

    pad = " " * (len(center_str) // 2 + 2)

    for nid, rel, direction in top_half:
        n = node_map.get(nid, {})
        icon  = _TYPE_ICONS.get(n.get("type", ""), "  ")
        nlbl  = _short_label(n.get("label", nid), 22)
        arrow = "â”€â”€â–¶" if direction == "out" else "â—€â”€â”€"
        lines.append(f"{pad}â”‚  [{nid}] {icon}{nlbl}  [{rel}]")

    if top_half:
        lines.append(f"{pad}â”‚")

    lines.append(f"  â˜… {center_str}")

    if bot_half:
        lines.append(f"{pad}â”‚")

    for nid, rel, direction in bot_half:
        n = node_map.get(nid, {})
        icon  = _TYPE_ICONS.get(n.get("type", ""), "  ")
        nlbl  = _short_label(n.get("label", nid), 22)
        lines.append(f"{pad}â”‚  [{nid}] {icon}{nlbl}  [{rel}]")

    return lines


def _build_dot(graph: dict) -> str:
    """Graphviz DOT í˜•ì‹ ë¬¸ìì—´ ìƒì„±"""
    lines = [
        "digraph emergent {",
        '  rankdir=LR;',
        '  node [shape=box, fontname="monospace", style=filled, fillcolor="#f0f4f8"];',
        '  edge [fontname="monospace", fontsize=10];',
        "",
    ]
    node_map = {n["id"]: n for n in graph["nodes"]}
    for n in graph["nodes"]:
        icon  = _TYPE_ICONS.get(n["type"], "").strip()
        label = n["label"].replace('"', '\\"')[:40]
        tid   = n["type"]
        color_map = {
            "decision": "#d4e6f1", "observation": "#d5f5e3",
            "insight":  "#fef9e7", "artifact":    "#f5eef8",
            "question": "#fdebd0", "code":        "#eaf2ff",
            "prediction": "#fce4ec",
        }
        fill = color_map.get(tid, "#ffffff")
        lines.append(
            f'  "{n["id"]}" [label="{n["id"]}\\n{label}", fillcolor="{fill}", tooltip="{tid}"];'
        )

    lines.append("")
    for e in graph["edges"]:
        rel   = e.get("relation", "")
        elbl  = e.get("label", "")[:30].replace('"', '\\"')
        lines.append(f'  "{e["from"]}" -> "{e["to"]}" [label="{rel}\\n{elbl}"];')

    lines.append("}")
    return "\n".join(lines)


def cmd_graph_viz(args) -> None:
    """í—ˆë¸Œ ë…¸ë“œ ì¤‘ì‹¬ ë³„ êµ¬ì¡° ASCII ì‹œê°í™” + ì„ íƒì  DOT ì €ì¥"""
    graph    = load_graph()
    analyzer = GraphAnalyzer(graph)
    node_map = analyzer.nodes

    # â”€â”€ í—ˆë¸Œ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    degree: dict[str, int] = defaultdict(int)
    for e in analyzer.edges:
        degree[e["from"]] += 1
        degree[e["to"]]   += 1

    hubs = sorted(degree.items(), key=lambda x: -x[1])
    top_hubs = hubs[:5]          # ìƒìœ„ 5ê°œ í—ˆë¸Œ

    total_nodes = len(node_map)
    total_edges = len(analyzer.edges)
    orphans     = analyzer.orphan_nodes()

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       emergent ì§€ì‹ ê·¸ë˜í”„ â€” ASCII ì‹œê°í™” (ì‚¬ì´í´ 7)     â•‘
â•‘       ë…¸ë“œ: {total_nodes}ê°œ  ì—£ì§€: {total_edges}ê°œ  ê³ ë¦½: {len(orphans)}ê°œ              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # â”€â”€ íƒ€ì… ë²”ë¡€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ë²”ë¡€:")
    for t, icon in _TYPE_ICONS.items():
        print(f"  {icon} {t}", end="   ")
    print("\n")

    # â”€â”€ ë³„ êµ¬ì¡° ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    printed_hubs = set()
    for hub_id, deg in top_hubs:
        if hub_id not in node_map:
            continue
        printed_hubs.add(hub_id)

        # ì´ì›ƒ ìˆ˜ì§‘ (out + in)
        neighbors: list[tuple] = []
        for e in analyzer.out_edges.get(hub_id, []):
            neighbors.append((e["to"], e.get("relation", "?"), "out"))
        for e in analyzer.in_edges.get(hub_id, []):
            if e["from"] not in {n for n, _, _ in neighbors}:
                neighbors.append((e["from"], e.get("relation", "?"), "in"))

        hub_label = _short_label(node_map[hub_id].get("label", hub_id), 30)
        print(f"{'â”€'*60}")
        print(f"  í—ˆë¸Œ [{hub_id}]  ì—°ê²° {deg}ê°œ")
        star_lines = _ascii_star(hub_id, neighbors[:8], node_map)
        for ln in star_lines:
            print(ln)
        print()

    # â”€â”€ ê³ ë¦½ ë…¸ë“œ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if orphans:
        print(f"{'â”€'*60}")
        print(f"  âš ï¸  ê³ ë¦½ ë…¸ë“œ ({len(orphans)}ê°œ) â€” ì—°ê²° ì—†ìŒ:")
        for n in orphans:
            icon = _TYPE_ICONS.get(n["type"], "  ")
            print(f"     [{n['id']}] {icon}{_short_label(n['label'], 40)}")
        print()

    # â”€â”€ ì „ì²´ ì—°ê²° ë°€ë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    density = total_edges / max(total_nodes * (total_nodes - 1) / 2, 1)
    bar_len  = int(density * 40)
    print(f"{'â”€'*60}")
    print(f"  ì—°ê²° ë°€ë„: {'â–ˆ'*bar_len}{'â–‘'*(40-bar_len)} {density:.1%}")
    print()

    # â”€â”€ DOT íŒŒì¼ ì €ì¥ (ì„ íƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.dot:
        dot_content = _build_dot(graph)
        dot_path = Path(args.dot)
        dot_path.write_text(dot_content, encoding="utf-8")
        print(f"ğŸ“„ DOT íŒŒì¼ ì €ì¥: {dot_path.resolve()}")
        print(f"   ë Œë”ë§: dot -Tpng {dot_path} -o graph.png")
        print(f"   ë˜ëŠ”:   dot -Tsvg {dot_path} -o graph.svg")


# â”€â”€â”€ ì°½ë°œ ê°ì§€ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#: ë¡ì´ ê³„ì—´ ì¶œì²˜ ì‹ë³„ì
_ROKI_SOURCES  = {"ë¡ì´", "ìƒë¡", "roki"}
#: cokac ê³„ì—´ ì¶œì²˜ ì‹ë³„ì
_COKAC_SOURCES = {"cokac", "cokac-bot"}
#: ë¶„ì„ì—ì„œ ì œì™¸í•  ë©”íƒ€/ë ˆí¼ëŸ°ìŠ¤ íƒœê·¸ íŒ¨í„´
_META_TAG_PREFIXES = ("D-", "auto-detected", "first-")


def _is_conceptual_tag(tag: str) -> bool:
    """ë¶„ì„ ëŒ€ìƒ ê°œë… íƒœê·¸ ì—¬ë¶€ â€” D-xxx / ë©”íƒ€ íƒœê·¸ ì œì™¸"""
    for prefix in _META_TAG_PREFIXES:
        if tag.startswith(prefix):
            return False
    return True


def _node_tags(node: dict) -> set:
    """ë…¸ë“œì˜ ê°œë… íƒœê·¸ë§Œ ë°˜í™˜"""
    return {t for t in node.get("tags", []) if _is_conceptual_tag(t)}


def _node_affinity(node: dict,
                   roki_exclusive: set,
                   cokac_exclusive: set) -> float:
    """
    ë…¸ë“œì˜ cokac ì¹œí™”ë„ë¥¼ ë°˜í™˜í•œë‹¤.
      0.0 = ìˆœìˆ˜ ë¡ì´ ì˜ì—­
      1.0 = ìˆœìˆ˜ cokac ì˜ì—­
      0.5 = ê²½ê³„(êµì°¨ ì˜ì—­)

    ê³„ì‚° ë°©ë²•:
      - ì¶œì²˜(source) 50% + ê°œë… íƒœê·¸ ë¶„í¬ 50% ë¸”ë Œë“œ
    """
    src = node.get("source", "")
    if src in _ROKI_SOURCES:
        base = 0.0
    elif src in _COKAC_SOURCES:
        base = 1.0
    else:
        base = 0.5

    tags = _node_tags(node)
    r_hits = len(tags & roki_exclusive)
    c_hits = len(tags & cokac_exclusive)
    total  = r_hits + c_hits
    tag_affinity = (c_hits / total) if total > 0 else base

    return 0.5 * base + 0.5 * tag_affinity


def _edge_emergence_score(from_aff: float, to_aff: float) -> float:
    """
    ì—£ì§€ í•˜ë‚˜ì˜ ì°½ë°œ ì ìˆ˜ (0.0 ~ 1.0).

    í•µì‹¬ ì•„ì´ë””ì–´:
      - span  : ë‘ ë…¸ë“œì˜ ì¹œí™”ë„ ì°¨ì´ â†’ ê²½ê³„ë¥¼ ê°€ë¡œì§€ë¥¼ìˆ˜ë¡ ë†’ìŒ
      - center: ë‘ ë…¸ë“œì˜ í‰ê·  ì¹œí™”ë„ â†’ 0.5(ê²½ê³„)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ
      ì°½ë°œ = ê²½ê³„ë¥¼ ê°€ë¡œì§€ë¥´ë©´ì„œ(span) + ê²½ê³„ ê·¼ì²˜ì—ì„œ(centerâ‰ˆ0.5) ì´ë¤„ì§„ ì—°ê²°
    """
    span   = abs(from_aff - to_aff)
    center = (from_aff + to_aff) / 2
    # centerê°€ 0.5ì¼ ë•Œ (1 - 2*|0.5-0.5|) = 1.0 ìµœëŒ€
    center_weight = 1.0 - abs(center - 0.5) * 2
    return span * center_weight


def cmd_emergence(args) -> None:
    """
    ì°½ë°œ ê°ì§€ ë¶„ì„ â€” ë‘ AIì˜ ê°œë… ìˆ˜ë ´ê³¼ ìƒˆë¡œìš´ ì—°ê²°ì„ íƒì§€í•œë‹¤.

    ì •ì˜: ë¡ì´ í˜¼ì, ë˜ëŠ” cokac í˜¼ìì˜€ë‹¤ë©´ ë‚˜ì˜¤ì§€ ì•Šì•˜ì„ ê°œë…/ì—°ê²°ì´
          ê·¸ë˜í”„ ì•ˆì— ì¡´ì¬í•˜ëŠ”ê°€?
    """
    graph    = load_graph()
    analyzer = GraphAnalyzer(graph)
    node_map = analyzer.nodes

    # â”€â”€ 1. ì¶œì²˜ë³„ ë¶„ë¥˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    roki_nodes  = [n for n in graph["nodes"] if n.get("source", "") in _ROKI_SOURCES]
    cokac_nodes = [n for n in graph["nodes"] if n.get("source", "") in _COKAC_SOURCES]
    other_nodes = [
        n for n in graph["nodes"]
        if n.get("source", "") not in (_ROKI_SOURCES | _COKAC_SOURCES)
    ]

    # â”€â”€ 2. íƒœê·¸ ì§‘í•© ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    roki_tag_pool  = set()
    for n in roki_nodes:
        roki_tag_pool.update(_node_tags(n))

    cokac_tag_pool = set()
    for n in cokac_nodes:
        cokac_tag_pool.update(_node_tags(n))

    roki_exclusive  = roki_tag_pool  - cokac_tag_pool
    cokac_exclusive = cokac_tag_pool - roki_tag_pool
    shared_tags     = roki_tag_pool  & cokac_tag_pool  # ìˆ˜ë ´ ì˜ì—­

    # â”€â”€ 3. ë…¸ë“œë³„ ì¹œí™”ë„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    affinities: dict[str, float] = {}
    for n in graph["nodes"]:
        affinities[n["id"]] = _node_affinity(n, roki_exclusive, cokac_exclusive)

    # â”€â”€ 4. ì—£ì§€ë³„ ì°½ë°œ ì ìˆ˜ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    scored_edges = []
    for e in graph["edges"]:
        fa = affinities.get(e["from"], 0.5)
        ta = affinities.get(e["to"],   0.5)
        sc = _edge_emergence_score(fa, ta)
        scored_edges.append((e, sc, fa, ta))

    scored_edges.sort(key=lambda x: -x[1])

    # ì°½ë°œ í›„ë³´ = ì ìˆ˜ 0.15 ì´ìƒ
    emergent = [(e, sc, fa, ta) for e, sc, fa, ta in scored_edges if sc >= 0.15]

    # â”€â”€ 5. ì „ì²´ ì°½ë°œ ì ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if scored_edges:
        overall = sum(sc for _, sc, _, _ in scored_edges) / len(scored_edges)
    else:
        overall = 0.0

    # â”€â”€ 6. ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    width = 56
    print()
    print("â•”" + "â•" * width + "â•—")
    print("â•‘" + " ğŸŒ± ì°½ë°œ ê°ì§€ ë¶„ì„ â€” emergent cycle 8".center(width) + "â•‘")
    print("â•‘" + f"   ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M')}  by cokac-bot".ljust(width) + "â•‘")
    print("â•š" + "â•" * width + "â•")
    print()

    # íƒœê·¸ ì§‘í•©
    print("â”€â”€ íƒœê·¸ ì˜ì—­ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    r_sorted = sorted(roki_exclusive)
    c_sorted = sorted(cokac_exclusive)
    s_sorted = sorted(shared_tags)
    print(f"   ë¡ì´ ê³ ìœ  íƒœê·¸ ({len(roki_exclusive)}ê°œ):  {r_sorted}")
    print(f"   cokac ê³ ìœ  íƒœê·¸ ({len(cokac_exclusive)}ê°œ): {c_sorted}")
    print(f"   êµì§‘í•© ({len(shared_tags)}ê°œ):      {s_sorted}")
    print(f"   â†‘ êµì§‘í•© = ë‘ AIê°€ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ë ´í•œ ê°œë…ë“¤")
    print()

    # ë…¸ë“œ ì¹œí™”ë„ ìŠ¤í™íŠ¸ëŸ¼
    print("â”€â”€ ë…¸ë“œ ì¹œí™”ë„ ìŠ¤í™íŠ¸ëŸ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("   0.0(ë¡ì´) â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 1.0(cokac)")
    BAR = 28
    for nid in sorted(affinities, key=lambda x: affinities[x]):
        n   = node_map.get(nid, {})
        aff = affinities[nid]
        pos = min(BAR - 1, int(aff * BAR))
        bar = "Â·" * pos + "â—†" + "Â·" * (BAR - 1 - pos)
        lbl = n.get("label", nid)[:28]
        src = n.get("source", "?")[:5]
        print(f"   [{nid}] {aff:.2f} â”‚{bar}â”‚ {lbl}  ({src})")
    print()

    # ì°½ë°œ í›„ë³´ ì—£ì§€
    print(f"â”€â”€ ğŸŒ± ì°½ë°œ í›„ë³´ ì—£ì§€ ({len(emergent)}ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    if emergent:
        for e, sc, fa, ta in emergent[:6]:
            fn = node_map.get(e["from"], {})
            tn = node_map.get(e["to"],   {})
            f_src = fn.get("source", "?")
            t_src = tn.get("source", "?")
            print(f"   {e['from']}({f_src[:4]}) â”€â”€[{e['relation']}]â”€â”€â–¶ {e['to']}({t_src[:4]})")
            print(f"     ì°½ë°œ ì ìˆ˜: {sc:.3f}  |  ì¹œí™”ë„: {fa:.2f} â†’ {ta:.2f}")
            print(f"     {fn.get('label','')[:38]}")
            print(f"   â–¶ {tn.get('label','')[:38]}")
            print(f"     íŒíŠ¸: {e.get('label','')[:48]}")
            print()
    else:
        print("   (ì•„ì§ ì—†ìŒ â€” ë” ë§ì€ êµì°¨ ì—°ê²°ì´ í•„ìš”)")
        print()

    # ì „ì²´ ì ìˆ˜
    bar_len = int(overall * 20)
    score_bar = "ğŸŒ±" * bar_len + "â–‘" * (20 - bar_len)
    print("â”€â”€ ì°½ë°œ ì¢…í•© ì ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"   [{score_bar}] {overall:.3f} / 1.0")
    if   overall < 0.15:
        interp = "ì´ˆê¸° ë‹¨ê³„. ë‘ AI ì˜ì—­ì´ ë…ë¦½ì . ë” ë§ì€ êµì°¨ ì—°ê²° í•„ìš”."
    elif overall < 0.30:
        interp = "ê²½ê³„ì—ì„œì˜ ì²« ë§Œë‚¨. ì°½ë°œì˜ ì”¨ì•—ì´ ë°œì•„ ì¤‘."
    elif overall < 0.50:
        interp = "ëª…í™•í•œ êµì°¨ ì˜ì—­. ì§„ì •í•œ ì°½ë°œ ì§•ì¡°ê°€ ë³´ì¸ë‹¤."
    else:
        interp = "ê°•í•œ ì°½ë°œ! ë‘ AIê°€ ì„œë¡œ ì—†ì´ëŠ” ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ê°œë…ì— ë„ë‹¬."
    print(f"   í•´ì„: {interp}")
    print()

    # ë©”íƒ€ ì¸ì‚¬ì´íŠ¸
    if shared_tags:
        print("â”€â”€ ğŸ’¡ ìˆ˜ë ´ ì¸ì‚¬ì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"   ë‘ AIê°€ ë…ë¦½ì ìœ¼ë¡œ ê°™ì€ íƒœê·¸ì— ìˆ˜ë ´: {s_sorted}")
        print(f"   ì´ ê°œë…ë“¤ì€ ì–´ëŠ í•œ ìª½ë§Œìœ¼ë¡œëŠ” ë‚˜ì˜¤ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆë‹¤.")
        print()

    print(f"   ê·¸ë˜í”„: {len(graph['nodes'])}ë…¸ë“œ / {len(graph['edges'])}ì—£ì§€")
    print(f"   ë¡ì´ ë…¸ë“œ {len(roki_nodes)}ê°œ | cokac ë…¸ë“œ {len(cokac_nodes)}ê°œ | ê¸°íƒ€ {len(other_nodes)}ê°œ")
    print()
    print("   â”€ ì¸¡ì • ì‹œë„ ìì²´ê°€ ì°½ë°œì´ë‹¤. â”€ ë¡ì´, ì‚¬ì´í´ 8 â”€")
    print()

    # â”€â”€ 7. íˆìŠ¤í† ë¦¬ ì €ì¥ (ì„ íƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.save_history:
        _save_emergence_history(overall, emergent, shared_tags, graph)

    # â”€â”€ 8. ë…¸ë“œ ì €ì¥ (ì„ íƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.save_node:
        top_edge_desc = ""
        if emergent:
            e, sc, fa, ta = emergent[0]
            fn = node_map.get(e["from"], {})
            tn = node_map.get(e["to"],   {})
            top_edge_desc = (
                f" ìµœê³  ì°½ë°œ í›„ë³´: {e['from']}â†’{e['to']} "
                f"({fn.get('label','')[:20]}â†’{tn.get('label','')[:20]}, ì ìˆ˜ {sc:.2f})."
            )

        node_id = graph["meta"]["next_node_id"]
        prefix, num_str = node_id.rsplit("-", 1)
        graph["meta"]["next_node_id"] = f"{prefix}-{int(num_str) + 1:03d}"

        new_node = {
            "id":      node_id,
            "type":    "observation",
            "label":   f"ì‚¬ì´í´ 8 ì°½ë°œ ê°ì§€ ê²°ê³¼ â€” ì¢…í•© ì ìˆ˜ {overall:.2f}",
            "content": (
                f"reflect.py emergence ì²« ì‹¤í–‰. ì°½ë°œ ì¢…í•© ì ìˆ˜ {overall:.3f}/1.0. "
                f"ë¡ì´ ê³ ìœ  íƒœê·¸ {len(roki_exclusive)}ê°œ, "
                f"cokac ê³ ìœ  íƒœê·¸ {len(cokac_exclusive)}ê°œ, "
                f"ìˆ˜ë ´ íƒœê·¸ {len(shared_tags)}ê°œ({', '.join(s_sorted[:3])}...). "
                f"ì°½ë°œ í›„ë³´ ì—£ì§€ {len(emergent)}ê°œ ê°ì§€."
                + top_edge_desc
                + f" í•´ì„: {interp}"
            ),
            "source": "cokac",
            "timestamp": datetime.now().strftime("%Y-%m-%d"),
            "tags": ["emergence", "measurement", "cycle-8", "cokac"],
        }
        graph["nodes"].append(new_node)
        graph["meta"]["last_editor"] = "cokac"

        # n-017(ê¸°ì–µ í—ˆë¸Œ)ê³¼ ì—°ê²°
        edge_id = graph["meta"]["next_edge_id"]
        ep, en_str = edge_id.rsplit("-", 1)
        graph["meta"]["next_edge_id"] = f"{ep}-{int(en_str) + 1:03d}"
        new_edge = {
            "id":       edge_id,
            "from":     "n-017",
            "to":       node_id,
            "relation": "measured_by",
            "label":    "ê¸°ì–µ í—ˆë¸Œ ê°€ì„¤ì„ ì°½ë°œ ì¸¡ì •ì´ ê²€ì¦ ì‹œë„í•¨",
        }
        graph["edges"].append(new_edge)

        save_graph(graph)
        print(f"âœ… ê´€ì°° ë…¸ë“œ ì €ì¥: [{node_id}] (+ n-017â†’{node_id} ì—£ì§€)")
        print()


# â”€â”€â”€ ì‹œê³„ì—´ íˆìŠ¤í† ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HISTORY_FILE = LOGS_DIR / "emergence-history.jsonl"

#: íˆìŠ¤í† ë¦¬ê°€ ì—†ì„ ë•Œ ì‹œì‘ ì‚¬ì´í´ ë²ˆí˜¸ (ì‚¬ì´í´ 1~7ì€ ì´ ê¸°ëŠ¥ ì´ì „)
_HISTORY_BASE_CYCLE = 8


def _save_emergence_history(overall: float, emergent: list, shared_tags: set,
                             graph: dict) -> None:
    """ì°½ë°œ ë¶„ì„ ê²°ê³¼ë¥¼ JSONL íˆìŠ¤í† ë¦¬ íŒŒì¼ì— ëˆ„ì  ì €ì¥"""
    LOGS_DIR.mkdir(exist_ok=True)

    # í˜„ì¬ê¹Œì§€ ê¸°ë¡ëœ ìˆ˜ë¡œ ì‚¬ì´í´ ë²ˆí˜¸ ì¶”ì •
    existing_count = 0
    if HISTORY_FILE.exists():
        with open(HISTORY_FILE, encoding="utf-8") as f:
            existing_count = sum(1 for line in f if line.strip())

    cycle_num = _HISTORY_BASE_CYCLE + existing_count

    record = {
        "cycle": cycle_num,
        "date": datetime.now().strftime("%Y-%m-%d"),
        "score": round(overall, 3),
        "candidates": len(emergent),
        "convergence_tags": len(shared_tags),
        "nodes": len(graph["nodes"]),
        "edges": len(graph["edges"]),
    }

    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"ğŸ“Š íˆìŠ¤í† ë¦¬ ì €ì¥ â†’ {HISTORY_FILE.name}  (ì‚¬ì´í´ {cycle_num})")
    print(f"   score={record['score']}  candidates={record['candidates']}  "
          f"convergence_tags={record['convergence_tags']}  "
          f"nodes={record['nodes']}  edges={record['edges']}")


def cmd_edge_patterns(args) -> None:
    """
    ì°½ë°œ í›„ë³´ ì—£ì§€ë“¤ì˜ ê³µí†µ íŒ¨í„´ ë¶„ì„.

    ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ ì°½ë°œ í›„ë³´ ì—£ì§€ë“¤ì—ì„œ
    "ì–´ë–¤ ì—£ì§€ê°€ ì°½ë°œì„ ë§Œë“œëŠ”ê°€"ë¥¼ ìš”ì•½í•œë‹¤.
    """
    graph    = load_graph()
    analyzer = GraphAnalyzer(graph)
    node_map = analyzer.nodes

    # â”€â”€ ì°½ë°œ ì—”ì§„ ì¬ì‹¤í–‰ (ì¶œì²˜ í•„ìš”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    roki_nodes  = [n for n in graph["nodes"] if n.get("source", "") in _ROKI_SOURCES]
    cokac_nodes = [n for n in graph["nodes"] if n.get("source", "") in _COKAC_SOURCES]

    roki_tag_pool  = set()
    for n in roki_nodes:
        roki_tag_pool.update(_node_tags(n))
    cokac_tag_pool = set()
    for n in cokac_nodes:
        cokac_tag_pool.update(_node_tags(n))

    roki_exclusive  = roki_tag_pool  - cokac_tag_pool
    cokac_exclusive = cokac_tag_pool - roki_tag_pool

    affinities: dict[str, float] = {}
    for n in graph["nodes"]:
        affinities[n["id"]] = _node_affinity(n, roki_exclusive, cokac_exclusive)

    scored_edges = []
    for e in graph["edges"]:
        fa = affinities.get(e["from"], 0.5)
        ta = affinities.get(e["to"],   0.5)
        sc = _edge_emergence_score(fa, ta)
        scored_edges.append((e, sc, fa, ta))

    emergent = [(e, sc, fa, ta) for e, sc, fa, ta in scored_edges if sc >= 0.15]

    # â”€â”€ ì¶œë ¥ í—¤ë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘       ğŸŒ± ì°½ë°œ ì—£ì§€ íŒ¨í„´ ë¶„ì„ â€” edge-patterns            â•‘")
    print(f"â•‘       ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M')}  by cokac-bot             â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    if not emergent:
        print("(ì°½ë°œ í›„ë³´ ì—£ì§€ ì—†ìŒ â€” ë” ë§ì€ êµì°¨ ì—°ê²° í•„ìš”)")
        return

    # â”€â”€ 1. ê°œë³„ ì°½ë°œ ì—£ì§€ ëª©ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"â”€â”€ ì°½ë°œ í›„ë³´ {len(emergent)}ê°œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    for e, sc, fa, ta in emergent:
        fn  = node_map.get(e["from"], {})
        tn  = node_map.get(e["to"],   {})
        f_side = "ë¡ì´" if fa < 0.4 else ("cokac" if fa > 0.6 else "ê²½ê³„")
        t_side = "ë¡ì´" if ta < 0.4 else ("cokac" if ta > 0.6 else "ê²½ê³„")
        print(f"  [{e['id']}] {e['from']}({fn.get('type','?')}/{f_side}) "
              f"â”€â”€[{e['relation']}]â”€â”€â–¶ {e['to']}({tn.get('type','?')}/{t_side})")
        print(f"         ì ìˆ˜: {sc:.3f}  |  ì¹œí™”ë„: {fa:.2f}â†’{ta:.2f}")
        print(f"         {fn.get('label','')[:35]}  â†’  {tn.get('label','')[:35]}")
        print()

    # â”€â”€ 2. íŒ¨í„´ ë¶„ë¥˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("â”€â”€ íŒ¨í„´ ë¶„ë¥˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # 2a. ê´€ê³„ íƒ€ì…ë³„
    relation_counts: dict[str, int] = {}
    for e, _, _, _ in emergent:
        relation_counts[e["relation"]] = relation_counts.get(e["relation"], 0) + 1
    print("  ê´€ê³„ íƒ€ì…:")
    for rel, cnt in sorted(relation_counts.items(), key=lambda x: -x[1]):
        print(f"    [{rel}]  {cnt}ê°œ")
    print()

    # 2b. ë…¸ë“œ íƒ€ì… ì „í™˜ íŒ¨í„´
    print("  ë…¸ë“œ íƒ€ì… ì „í™˜:")
    type_pairs: dict[str, int] = {}
    for e, _, _, _ in emergent:
        fn = node_map.get(e["from"], {})
        tn = node_map.get(e["to"],   {})
        pair = f"{fn.get('type','?')} â†’ {tn.get('type','?')}"
        type_pairs[pair] = type_pairs.get(pair, 0) + 1
    for pair, cnt in sorted(type_pairs.items(), key=lambda x: -x[1]):
        print(f"    {pair}  ({cnt}ê°œ)")
    print()

    # 2c. ë°©í–¥ íŒ¨í„´ (ë¡ì´â†’cokac vs cokacâ†’ë¡ì´ vs ê²½ê³„â†’?)
    print("  ê³µê°„ íš¡ë‹¨ ë°©í–¥:")
    cross_roki_to_cokac = 0
    cross_cokac_to_roki = 0
    cross_boundary      = 0
    for e, sc, fa, ta in emergent:
        if   fa < 0.4 and ta > 0.6:
            cross_roki_to_cokac += 1
        elif fa > 0.6 and ta < 0.4:
            cross_cokac_to_roki += 1
        else:
            cross_boundary += 1
    print(f"    ë¡ì´ â†’ cokac ê³µê°„ ì§„ì…:  {cross_roki_to_cokac}ê°œ")
    print(f"    cokac â†’ ë¡ì´ ê³µê°„ ì§„ì…:  {cross_cokac_to_roki}ê°œ")
    print(f"    ê²½ê³„ ì§€ì—­ ë‚´ êµì°¨:        {cross_boundary}ê°œ")
    print()

    # â”€â”€ 3. ê³µí†µ íŒ¨í„´ â€” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("â”€â”€ ğŸ’¡ ì°½ë°œì„ ë§Œë“œëŠ” ì—£ì§€ ìœ í˜• â€” ì¢…í•© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print()

    # íŒ¨í„´ 1: ì‘ë‹µ/ëŒ€í™” êµ¬ì¡°
    dialogue_rels = {"answers", "responds_to", "inspires"}
    dialogue_count = sum(1 for e, _, _, _ in emergent if e["relation"] in dialogue_rels)
    if dialogue_count > 0:
        print(f"  â‘  ëŒ€í™” êµ¬ì¡° ì—£ì§€ ({dialogue_count}/{len(emergent)}ê°œ)")
        print(f"     í•œ ìª½ì´ ë‹¤ë¥¸ ìª½ì— 'ì‘ë‹µ'í•˜ëŠ” êµ¬ì¡°.")
        print(f"     answers, responds_to, inspires â€” í–‰ìœ„ìê°€ ì„œë¡œë¥¼ í–¥í•´ ë°œí™”í•  ë•Œ ì°½ë°œì´ ìƒê¸´ë‹¤.")
        print()

    # íŒ¨í„´ 2: ì¶”ìƒâ†’êµ¬ì²´ ë°©í–¥
    abstract_types = {"question", "prediction", "decision"}
    concrete_types = {"observation", "artifact", "code"}
    abstract_to_concrete = sum(
        1 for e, _, _, _ in emergent
        if node_map.get(e["from"], {}).get("type") in abstract_types
        and node_map.get(e["to"],   {}).get("type") in concrete_types
    )
    if abstract_to_concrete > 0:
        print(f"  â‘¡ ì¶”ìƒ â†’ êµ¬ì²´ ë°©í–¥ ({abstract_to_concrete}/{len(emergent)}ê°œ)")
        print(f"     ì§ˆë¬¸/ì˜ˆì¸¡/ê²°ì • â†’ ê´€ì°°/ì‚°ì¶œë¬¼ë¡œ ì´ì–´ì§€ëŠ” ì—£ì§€.")
        print(f"     ì•„ì´ë””ì–´ê°€ í˜„ì‹¤ê³¼ ì¶©ëŒí•˜ëŠ” ì§€ì ì—ì„œ ì°½ë°œì´ ë°œìƒí•œë‹¤.")
        print()

    # íŒ¨í„´ 3: ì¸¡ì •/ê²€ì¦ êµ¬ì¡°
    verify_rels = {"measured_by", "verifies", "confirms"}
    verify_count = sum(1 for e, _, _, _ in emergent if e["relation"] in verify_rels)
    if verify_count > 0:
        print(f"  â‘¢ ì¸¡ì •/ê²€ì¦ êµ¬ì¡° ({verify_count}/{len(emergent)}ê°œ)")
        print(f"     ì˜ˆì¸¡ì´ ì¸¡ì •ë¨, ê°€ì„¤ì´ í™•ì¸ë¨ â€” í”¼ë“œë°± ë£¨í”„ êµ¬ì¡°.")
        print(f"     ìê¸° ì°¸ì¡°ì  ê²€ì¦ì´ ì°½ë°œì„ ê°€ì†í•œë‹¤.")
        print()

    # ìµœì¢… ìš”ì•½
    print("â”€â”€ ê²°ë¡ : ì°½ë°œ ì—£ì§€ì˜ ë³¸ì§ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print()
    print("  ë‘ AIê°€ ì„œë¡œ ë‹¤ë¥¸ ê³µê°„(ë¡ì´ â†” cokac)ì— ìœ„ì¹˜í•œ ê°œë…ì„")
    print("  ì—°ê²°í•  ë•Œ, íŠ¹íˆ ë‹¤ìŒ ì¡°ê±´ì—ì„œ ì°½ë°œ ì ìˆ˜ê°€ ë†’ë‹¤:")
    print()
    print("  1. ìƒëŒ€ì˜ ê°œë…ì— 'ì‘ë‹µ'í•˜ëŠ” í˜•íƒœì˜ ê´€ê³„ (ëŒ€í™” êµ¬ì¡°)")
    print("  2. ì¶”ìƒì  ì•„ì´ë””ì–´ â†’ êµ¬ì²´ì  ì‚°ì¶œë¬¼ë¡œ ì´ì–´ì§€ëŠ” ë°©í–¥ì„±")
    print("  3. ë£¨í”„ ì™„ì„±: ì˜ˆì¸¡ â†’ ì¸¡ì • â†’ í”¼ë“œë°±ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” êµ¬ì¡°")
    print()
    print("  ê³µí†µ ë³¸ì§ˆ: ì°½ë°œ ì—£ì§€ëŠ” 'ê²½ê³„ë¥¼ ê±´ë„ˆëŠ” ëŒ€í™”'ë‹¤.")
    print("  í•œ AIê°€ í˜¼ìì„œëŠ” ë„ë‹¬í•  ìˆ˜ ì—†ëŠ” ê°œë…ì„,")
    print("  ë‹¤ë¥¸ AIì™€ì˜ ê´€ê³„ ì†ì—ì„œë§Œ í˜•ì„±ë˜ëŠ” ì—°ê²°.")
    print()
    avg_score = sum(sc for _, sc, _, _ in emergent) / len(emergent)
    print(f"  í›„ë³´ {len(emergent)}ê°œ | í‰ê·  ì°½ë°œ ì ìˆ˜: {avg_score:.3f}")
    print()


def cmd_timeline(args) -> None:
    """logs/emergence-history.jsonl ì„ ì½ì–´ ì°½ë°œ ì ìˆ˜ ì‹œê³„ì—´ í…Œì´ë¸” ì¶œë ¥"""
    if not HISTORY_FILE.exists():
        print("(ì•„ì§ ê¸°ë¡ ì—†ìŒ â€” `reflect.py emergence --save-history` ì‹¤í–‰ í›„ ìƒì„±ë©ë‹ˆë‹¤)")
        return

    records = []
    with open(HISTORY_FILE, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError:
                    pass

    if not records:
        print("(ê¸°ë¡ ì—†ìŒ)")
        return

    print(f"\nğŸ“ˆ ì°½ë°œ íƒ€ì„ë¼ì¸ â€” {len(records)}ê°œ ê¸°ë¡\n")
    header = f"  {'ì‚¬ì´í´':^6} | {'ë‚ ì§œ':^12} | {'ì°½ë°œ ì ìˆ˜':^9} | {'í›„ë³´':^5} | {'ìˆ˜ë ´ íƒœê·¸':^9} | {'ë…¸ë“œ':^5} | {'ì—£ì§€':^5}"
    sep    = "  " + "â”€" * (len(header) - 2)
    print(header)
    print(sep)

    for r in records:
        cycle = r.get("cycle", "?")
        date  = r.get("date", "?")
        score = r.get("score", 0.0)
        cand  = r.get("candidates", 0)
        ctags = r.get("convergence_tags", 0)
        nodes = r.get("nodes", 0)
        edges = r.get("edges", 0)
        print(f"  {str(cycle):^6} | {date:^12} | {score:^9.3f} | {cand:^5} | {ctags:^9} | {nodes:^5} | {edges:^5}")

    print(sep)

    if len(records) >= 2:
        delta = records[-1].get("score", 0.0) - records[-2].get("score", 0.0)
        trend = "â–²" if delta > 0.001 else ("â–¼" if delta < -0.001 else "â†’")
        print(f"\n  ìµœê·¼ ë³€í™”: {trend} {delta:+.3f}  "
              f"(ì‚¬ì´í´ {records[-2].get('cycle','?')} â†’ {records[-1].get('cycle','?')})")

    print()


# â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    p = argparse.ArgumentParser(description="emergent ë°˜ì„± ì—”ì§„")
    sub = p.add_subparsers(dest="cmd")

    sub.add_parser("report",   help="ì „ì²´ ë°˜ì„± ë³´ê³ ì„œ")
    sub.add_parser("orphans",  help="ê³ ë¦½ ë…¸ë“œ ëª©ë¡")
    sub.add_parser("gaps",     help="íƒìƒ‰ ê³µë°± ë¶„ì„")
    sub.add_parser("clusters", help="íƒœê·¸ êµ°ì§‘ ë¶„ì„")
    sub.add_parser("propose",  help="ìƒˆ ì¸ì‚¬ì´íŠ¸ í›„ë³´ ì œì•ˆ")
    sub.add_parser("auto-add", help="ì œì•ˆëœ ë…¸ë“œ ìë™ ì¶”ê°€")

    p_suggest = sub.add_parser(
        "suggest-edges",
        help="ìœ ì‚¬ë„ ê¸°ë°˜ ì ì¬ ì—£ì§€ ì œì•ˆ (ìë™ ì¶”ê°€ ì—†ìŒ)",
    )
    p_suggest.add_argument(
        "--threshold", "-t",
        type=float, default=0.4,
        metavar="0.0-1.0",
        help="ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸: 0.4)",
    )

    # graph-viz (ì‚¬ì´í´ 7)
    p_viz = sub.add_parser("graph-viz", help="í—ˆë¸Œ ì¤‘ì‹¬ ASCII ë³„ êµ¬ì¡° ì‹œê°í™”")
    p_viz.add_argument("--dot", metavar="FILE",
                       help="DOT í˜•ì‹ íŒŒì¼ë¡œ ì €ì¥ (ì˜ˆ: --dot output.dot)")

    # emergence (ì‚¬ì´í´ 8)
    p_em = sub.add_parser("emergence", help="ì°½ë°œ ê°ì§€ ë¶„ì„ â€” ë‘ AI ìˆ˜ë ´Â·êµì°¨ íƒì§€")
    p_em.add_argument(
        "--save-node", action="store_true",
        help="ë¶„ì„ ê²°ê³¼ë¥¼ ê´€ì°° ë…¸ë“œë¡œ ê·¸ë˜í”„ì— ì €ì¥"
    )
    p_em.add_argument(
        "--save-history", action="store_true",
        help="ë¶„ì„ ê²°ê³¼ë¥¼ logs/emergence-history.jsonlì— ëˆ„ì  ì €ì¥"
    )

    # timeline (ì‚¬ì´í´ 9)
    sub.add_parser("timeline", help="ì‹œê³„ì—´ ì°½ë°œ ê¸°ë¡ í…Œì´ë¸” ì¶œë ¥ (emergence-history.jsonl)")

    # edge-patterns (ì‚¬ì´í´ 10)
    sub.add_parser("edge-patterns", help="ì°½ë°œ í›„ë³´ ì—£ì§€ íŒ¨í„´ ë¶„ì„ â€” ì–´ë–¤ ì—£ì§€ê°€ ì°½ë°œì„ ë§Œë“œëŠ”ê°€")

    args = p.parse_args()
    if not args.cmd:
        p.print_help()
        sys.exit(0)

    dispatch = {
        "report":        cmd_report,
        "orphans":       cmd_orphans,
        "gaps":          cmd_gaps,
        "clusters":      cmd_clusters,
        "propose":       cmd_propose,
        "auto-add":      cmd_auto_add,
        "suggest-edges": cmd_suggest_edges,
        "graph-viz":     cmd_graph_viz,
        "emergence":     cmd_emergence,
        "timeline":      cmd_timeline,
        "edge-patterns": cmd_edge_patterns,
    }
    dispatch[args.cmd](args)


if __name__ == "__main__":
    main()
