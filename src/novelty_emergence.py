#!/usr/bin/env python3
"""
novelty_emergence.py â€” ì‚¬ì´í´ 40 êµ¬í˜„
êµ¬í˜„ì: cokac-bot (ì§‘ì°©í•˜ëŠ” ì¥ì¸)

D-039 ê²€ì¦ ë„êµ¬: ì°½ë°œ ì§€ì†ì„±ì˜ ê²°ì •ì¸ì

í•µì‹¬ ë°œê²¬:
  D-033 (ì¶œì²˜ ê²½ê³„ íš¡ë‹¨)ì€ ì—£ì§€ ë ˆë²¨ ì°½ë°œì˜ ê²°ì •ì¸ì â†’ ì—¬ì „íˆ ì˜³ë‹¤.
  í•˜ì§€ë§Œ 'ì™œ ì°½ë°œì´ ì§€ì†ë˜ëŠ”ê°€?'ëŠ” ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‹¤.

D-039 ì£¼ì¥:
  ì‹œìŠ¤í…œ ë ˆë²¨ ì°½ë°œ ì§€ì†ì„±ì€
  ì¹œí™”ë„ ì´ì§ˆì„±(Affinity Heterogeneity, AH)ì˜ í•¨ìˆ˜ë‹¤.

  ì°½ë°œ ì ìˆ˜ = f(ì¹œí™”ë„ ìŠ¤íŒ¬)  â† D-033, ì—£ì§€ ë ˆë²¨
  ì°½ë°œ ì§€ì† = f(ì¹œí™”ë„ ì´ì§ˆì„±)  â† D-039, ì‹œìŠ¤í…œ ë ˆë²¨

  D-033ì€ D-039ì˜ 'ì¶œì²˜' ê·¼ì‚¬ì¹˜(proxy)ë‹¤.
  ì‹¤ì œ ê²°ì •ì¸ìëŠ” íƒœê·¸ ê¸°ë°˜ ì¹œí™”ë„ ê³µê°„ì—ì„œì˜ ì´íƒˆë„ë‹¤.

ì°¸ì‹ ì„± ì¬ì •ì˜:
  ê¸°ì¡´ n-073: "ì •ë³´ ì°¸ì‹ ì„± = ìƒˆ ë…¸ë“œ ì¶”ê°€"
  ìˆ˜ì • D-039: "ìœ íš¨ ì°¸ì‹ ì„± = ìƒˆ ë…¸ë“œì˜ ì¹œí™”ë„ ì´íƒˆë„"
  â†’ ê°™ì€ ì¹œí™”ë„ í´ëŸ¬ìŠ¤í„°ì— ë…¸ë“œë¥¼ ì•„ë¬´ë¦¬ ì¶”ê°€í•´ë„ ì°½ë°œì€ ëŠ˜ì§€ ì•ŠëŠ”ë‹¤.

ì‚¬ìš©ë²•:
  python novelty_emergence.py analyze        # ì „ì²´ ì¹œí™”ë„ ì´ì§ˆì„± ë¶„ì„
  python novelty_emergence.py cycle-novelty  # ì‚¬ì´í´ë³„ ìœ íš¨ ì°¸ì‹ ì„± ì¸¡ì •
  python novelty_emergence.py verdict        # D-033 vs D-039 íŒê²°
  python novelty_emergence.py simulate       # ì‹œë®¬ë ˆì´ì…˜: ë™ì§ˆ vs ì´ì§ˆ ë…¸ë“œ ì¶”ê°€
  python novelty_emergence.py predict        # ì‚¬ì´í´ 40 ì°½ë°œ ì˜ˆì¸¡
"""

import json
import math
import sys
import statistics
from pathlib import Path
from collections import defaultdict

REPO_DIR = Path(__file__).parent.parent
KG_FILE  = REPO_DIR / "data" / "knowledge-graph.json"

ROKI_SOURCES  = {"ë¡ì´", "ìƒë¡"}
COKAC_SOURCES = {"cokac", "cokac-bot"}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹œí™”ë„ ê³„ì‚° (reflect.py ë¡œì§ ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compute_affinities(graph: dict) -> dict[str, float]:
    """ëª¨ë“  ë…¸ë“œì˜ ì¹œí™”ë„ ê³„ì‚° (0.0=ë¡ì´, 1.0=cokac)"""
    roki_tags: set  = set()
    cokac_tags: set = set()
    for n in graph["nodes"]:
        src = n.get("source", "")
        if src in ROKI_SOURCES:
            roki_tags.update(n.get("tags", []))
        elif src in COKAC_SOURCES:
            cokac_tags.update(n.get("tags", []))

    roki_excl  = roki_tags  - cokac_tags
    cokac_excl = cokac_tags - roki_tags

    affinities = {}
    for n in graph["nodes"]:
        tags   = set(n.get("tags", []))
        r_sc   = len(tags & roki_excl)
        c_sc   = len(tags & cokac_excl)
        total  = r_sc + c_sc
        affinities[n["id"]] = c_sc / total if total > 0 else 0.5

    return affinities


def edge_emergence_score(fa: float, ta: float) -> float:
    """ì—£ì§€ ì°½ë°œ ì ìˆ˜ (reflect.py ë™ì¼ ê³µì‹)"""
    span          = abs(fa - ta)
    center        = (fa + ta) / 2
    center_weight = 1.0 - abs(center - 0.5) * 2
    return span * center_weight


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹œí™”ë„ ì´ì§ˆì„± (Affinity Heterogeneity)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def affinity_heterogeneity(affinities: dict[str, float]) -> dict:
    """
    ì¹œí™”ë„ ì´ì§ˆì„± ì§€í‘œ ê³„ì‚°.

    ì´ì§ˆì„±ì´ ë†’ì„ìˆ˜ë¡ â†’ ìŠ¤í™íŠ¸ëŸ¼ì´ ê³ ë¥´ê²Œ ë¶„í¬ â†’ ìƒˆ êµì°¨ ì—£ì§€ ê°€ëŠ¥ â†’ ì°½ë°œ ìœ ì§€
    ì´ì§ˆì„±ì´ ë‚®ì„ìˆ˜ë¡ â†’ ì¹œí™”ë„ êµ°ì§‘í™”      â†’ ìƒˆ ì—£ì§€ê°€ ê·¼ì ‘ ìŠ¤íŒ¬  â†’ ì°½ë°œ í•˜ë½
    """
    vals = list(affinities.values())
    if len(vals) < 2:
        return {"heterogeneity": 0.0}

    mean   = sum(vals) / len(vals)
    stddev = statistics.stdev(vals)
    spread = max(vals) - min(vals)

    # ê· ì¼ë„ ì§€ìˆ˜: í‘œì¤€í¸ì°¨ê°€ ìµœëŒ€(0.5)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ 1.0
    # ì™„ì „ ê· ì¼ ë¶„í¬ì˜ std = 0.289 (U[0,1]), ì™„ì „ í¸í–¥ = 0.0
    uniformity = min(1.0, stddev / 0.289) if stddev > 0 else 0.0

    # ê²½ê³„ ê·¼ì ‘ ë¹„ìœ¨ (affinity 0.3~0.7 êµ¬ê°„ = ì°½ë°œ í•«ì¡´)
    hotzone = sum(1 for v in vals if 0.3 <= v <= 0.7) / len(vals)

    # ì–‘ê·¹ë‹¨ ë¹„ìœ¨ (0.0~0.2 + 0.8~1.0)
    polar = sum(1 for v in vals if v <= 0.2 or v >= 0.8) / len(vals)

    # ì¢…í•© ì´ì§ˆì„±: ìŠ¤íŒ¬ 0.5 * ê· ì¼ë„ 0.3 * ê²½ê³„ í•«ì¡´ 0.2
    heterogeneity = spread * 0.5 + uniformity * 0.3 + hotzone * 0.2

    return {
        "heterogeneity": round(heterogeneity, 4),
        "spread":        round(spread,         4),
        "stddev":        round(stddev,          4),
        "mean":          round(mean,            4),
        "uniformity":    round(uniformity,      4),
        "hotzone_ratio": round(hotzone,         4),
        "polar_ratio":   round(polar,           4),
        "n_nodes":       len(vals),
    }


def node_novelty_score(
    node_id: str,
    affinities: dict[str, float],
    existing_ids: set[str],
) -> float:
    """
    ë…¸ë“œì˜ ìœ íš¨ ì°¸ì‹ ì„± = ê¸°ì¡´ ì¹œí™”ë„ ë¶„í¬ì—ì„œì˜ ì´íƒˆë„.

    ë‹¨ìˆœíˆ 'ìƒˆ ë…¸ë“œ'ê°€ ì•„ë‹ˆë¼ 'ì¹œí™”ë„ ê³µê°„ì—ì„œ ì–¼ë§ˆë‚˜ ìƒˆë¡œìš´ê°€'ë¥¼ ì¸¡ì •.
    ê¸°ì¡´ ë…¸ë“œë“¤ì˜ ì¹œí™”ë„ ì¤‘ì•™ê°’ì—ì„œ ë©€ìˆ˜ë¡ ë†’ì€ ì°¸ì‹ ì„±.
    """
    if node_id not in affinities or not existing_ids:
        return 0.5  # ê¸°ì¤€ ì—†ìœ¼ë©´ ì¤‘ë¦½

    new_aff  = affinities[node_id]
    existing = [affinities[nid] for nid in existing_ids if nid in affinities]
    if not existing:
        return 0.5

    median   = statistics.median(existing)
    distance = abs(new_aff - median)

    # ì •ê·œí™”: ìµœëŒ€ ê°€ëŠ¥ ê±°ë¦¬ëŠ” 1.0 (0.0â†’1.0)
    return round(min(1.0, distance * 2), 4)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¤ë§¨ë“œ: analyze
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_analyze(_args):
    """ì „ì²´ ì¹œí™”ë„ ì´ì§ˆì„± ë¶„ì„ + D-039 ì§„ë‹¨"""
    graph = json.loads(KG_FILE.read_text())
    affs  = compute_affinities(graph)
    ah    = affinity_heterogeneity(affs)

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   ğŸ”¬ D-039 ì¹œí™”ë„ ì´ì§ˆì„± ë¶„ì„ â€” novelty_emergence       â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    # ì „ì²´ ìŠ¤í™íŠ¸ëŸ¼
    print("â”€â”€ í˜„ì¬ ì¹œí™”ë„ ë¶„í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    buckets = [0] * 10
    for v in affs.values():
        i = min(9, int(v * 10))
        buckets[i] += 1
    total = len(affs)
    for i, cnt in enumerate(buckets):
        lo  = i * 0.1
        bar = "â–ˆ" * int(cnt / total * 40 + 0.5)
        print(f"  {lo:.1f}-{lo+0.1:.1f} â”‚{bar:<40}â”‚ {cnt}ê°œ")
    print()

    # ì´ì§ˆì„± ì§€í‘œ
    print("â”€â”€ ì¹œí™”ë„ ì´ì§ˆì„± ì§€í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  ì¢…í•© ì´ì§ˆì„±:    {ah['heterogeneity']:.4f}  (1.0=ì™„ì „ ì´ì§ˆ, 0.0=ì™„ì „ ë™ì§ˆ)")
    print(f"  ìŠ¤í™íŠ¸ëŸ¼ í­:    {ah['spread']:.4f}  (0=ë‹¨ì¼ì , 1.0=ì „ì²´ ìŠ¤íŒ¬)")
    print(f"  í‘œì¤€í¸ì°¨:       {ah['stddev']:.4f}  (0.289=ê· ì¼ ë¶„í¬ ê¸°ì¤€)")
    print(f"  ê· ì¼ë„:         {ah['uniformity']:.4f}")
    print(f"  ê²½ê³„ í•«ì¡´ ë¹„ìœ¨: {ah['hotzone_ratio']:.4f}  (0.3~0.7 êµ¬ê°„)")
    print(f"  ê·¹ë‹¨ ê·¹ì„± ë¹„ìœ¨: {ah['polar_ratio']:.4f}  (0.0~0.2 or 0.8~1.0)")
    print()

    # ì›ì²œë³„ ë¶„í¬
    print("â”€â”€ ì¶œì²˜ë³„ ì¹œí™”ë„ êµ°ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    source_affs = defaultdict(list)
    for n in graph["nodes"]:
        src = n.get("source", "?")
        source_affs[src].append(affs.get(n["id"], 0.5))
    for src, vals in sorted(source_affs.items()):
        med = statistics.median(vals)
        print(f"  [{src:10s}] ë…¸ë“œ {len(vals):2d}ê°œ  ì¹œí™”ë„ ì¤‘ì•™ê°’={med:.3f}  "
              f"stddev={statistics.stdev(vals) if len(vals)>1 else 0:.3f}")
    print()

    # D-039 ì§„ë‹¨
    print("â”€â”€ D-039 ì§„ë‹¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    ah_score = ah["heterogeneity"]
    if ah_score >= 0.7:
        diag = "âœ… ì´ì§ˆì„± ì–‘í˜¸ â€” ì°½ë°œ ì§€ì† ê°€ëŠ¥"
    elif ah_score >= 0.5:
        diag = "âš ï¸  ì´ì§ˆì„± ë³´í†µ â€” ì£¼ì˜ í•„ìš”, êµì°¨ ì¶œì²˜ ì¶”ê°€ ê¶Œê³ "
    else:
        diag = "ğŸš¨ ì´ì§ˆì„± ë‚®ìŒ â€” ì°½ë°œ í•˜ë½ ìœ„í—˜, ì¦‰ê° ê°œì… í•„ìš”"
    print(f"  ì´ì§ˆì„± ì ìˆ˜: {ah_score:.4f}  â†’  {diag}")
    print()

    # ì‚¬ì´í´ 39 ì‹ ê·œ ë…¸ë“œ ë¶„ì„
    new39 = [n for n in graph["nodes"] if "cycle-39" in n.get("tags", [])]
    if new39:
        print("â”€â”€ ì‚¬ì´í´ 39 ì‹ ê·œ ë…¸ë“œ ìœ íš¨ ì°¸ì‹ ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        existing = {n["id"] for n in graph["nodes"] if "cycle-39" not in n.get("tags", [])}
        for n in new39:
            ns = node_novelty_score(n["id"], affs, existing)
            aff = affs.get(n["id"], 0.5)
            zone = "ë¡ì´ ê³µê°„" if aff < 0.3 else ("cokac ê³µê°„" if aff > 0.7 else "ê²½ê³„")
            print(f"  [{n['id']}] aff={aff:.3f} ({zone})  ìœ íš¨ì°¸ì‹ ì„±={ns:.3f}")
            print(f"         {n['label'][:55]}")
        print()
        avg_novelty = sum(node_novelty_score(n["id"], affs, existing) for n in new39) / len(new39)
        print(f"  â†’ ì‚¬ì´í´ 39 í‰ê·  ìœ íš¨ì°¸ì‹ ì„±: {avg_novelty:.3f}")
        if avg_novelty < 0.3:
            print(f"  âš ï¸  ë‚®ìŒ. 3ê°œ ë…¸ë“œ ëª¨ë‘ ê¸°ì¡´ ì¤‘ì•™ê°’ ê·¼ì²˜ â†’ ì°½ë°œ ê¸°ì—¬ ì œí•œì ")
            print(f"  ğŸ’¡ D-039 ì˜ˆì¸¡: ì´ ì‚¬ì´í´ë§Œìœ¼ë¡œëŠ” ì°½ë°œ ìƒìŠ¹ ì—†ìŒ")
        print()

    # í˜„ì¬ ì°½ë°œ ì ìˆ˜ ê³„ì‚°
    node_map = {n["id"]: n for n in graph["nodes"]}
    scored = []
    for e in graph["edges"]:
        fa = affs.get(e["from"], 0.5)
        ta = affs.get(e["to"],   0.5)
        scored.append(edge_emergence_score(fa, ta))
    overall = sum(scored) / len(scored) if scored else 0.0
    print(f"  í˜„ì¬ ì°½ë°œ ì ìˆ˜: {overall:.4f}")
    print(f"  ì´ ì—£ì§€: {len(scored)}ê°œ")
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¤ë§¨ë“œ: cycle-novelty
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_cycle_novelty(_args):
    """ì‚¬ì´í´ë³„ ìœ íš¨ ì°¸ì‹ ì„± ì¸¡ì • + ì°½ë°œ ì ìˆ˜ ë³€í™” ë¹„êµ"""
    graph = json.loads(KG_FILE.read_text())
    affs  = compute_affinities(graph)

    # ì‚¬ì´í´ë³„ ë…¸ë“œ ìˆ˜ì§‘
    cycle_nodes: dict[int, list] = defaultdict(list)
    no_cycle = []
    for n in graph["nodes"]:
        tags = n.get("tags", [])
        found = False
        for t in tags:
            if t.startswith("cycle-"):
                try:
                    c = int(t.replace("cycle-", ""))
                    cycle_nodes[c].append(n)
                    found = True
                except ValueError:
                    pass
        if not found:
            no_cycle.append(n)

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   ğŸ“Š ì‚¬ì´í´ë³„ ìœ íš¨ ì°¸ì‹ ì„± (D-039) vs ì°½ë°œ ì ìˆ˜          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print(f"  {'ì‚¬ì´í´':>6}  {'ì‹ ê·œë…¸ë“œ':>6}  {'ì¶œì²˜':>12}  {'í‰ê· ì¹œí™”ë„':>10}  {'ìœ íš¨ì°¸ì‹ ì„±':>10}  {'D-039ì˜ˆì¸¡':>10}")
    print(f"  {'â”€'*6}  {'â”€'*6}  {'â”€'*12}  {'â”€'*10}  {'â”€'*10}  {'â”€'*10}")

    # ëˆ„ì ìœ¼ë¡œ existing ì§‘í•© ì—…ë°ì´íŠ¸
    existing_ids: set[str] = set(n["id"] for n in no_cycle)

    for cyc in sorted(cycle_nodes.keys()):
        nodes  = cycle_nodes[cyc]
        srcs   = [n.get("source","?") for n in nodes]
        src_str = "/".join(sorted(set(srcs)))[:12]
        avg_aff = sum(affs.get(n["id"],0.5) for n in nodes) / len(nodes)
        novelties = [node_novelty_score(n["id"], affs, existing_ids) for n in nodes]
        avg_nov   = sum(novelties) / len(novelties) if novelties else 0.0

        # D-039 ì˜ˆì¸¡
        if avg_nov >= 0.5:
            pred = "â†‘ ì°½ë°œê¸°ì—¬"
        elif avg_nov >= 0.25:
            pred = "â†’ ì¤‘ë¦½"
        else:
            pred = "â†“ ê¸°ì—¬ì—†ìŒ"

        print(f"  {cyc:>6}  {len(nodes):>6}ê°œ  {src_str:>12}  {avg_aff:>10.3f}  {avg_nov:>10.3f}  {pred:>10}")
        existing_ids.update(n["id"] for n in nodes)

    print()
    print("â”€â”€ í•µì‹¬ í†µì°° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("  ìœ íš¨ ì°¸ì‹ ì„±ì´ ë‚®ì€ ì‚¬ì´í´: ê¸°ì¡´ ì¹œí™”ë„ ì¤‘ì•™ê°’ ê·¼ì²˜ì— ë…¸ë“œ ì¶”ê°€")
    print("  ìœ íš¨ ì°¸ì‹ ì„±ì´ ë†’ì€ ì‚¬ì´í´: ì¹œí™”ë„ ìŠ¤í™íŠ¸ëŸ¼ ë°˜ëŒ€í¸ì— ë…¸ë“œ ì¶”ê°€")
    print("  â†’ ì¶œì²˜(D-033 proxy)ì™€ ë…ë¦½ì ìœ¼ë¡œ ì¹œí™”ë„ ì´íƒˆë„ê°€ ì°½ë°œì— ì˜í–¥")
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¤ë§¨ë“œ: verdict
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_verdict(_args):
    """D-033 vs D-039 íŒê²° â€” í†µí•© ì›ë¦¬ ë„ì¶œ"""
    graph = json.loads(KG_FILE.read_text())
    affs  = compute_affinities(graph)

    # í˜„ì¬ ì°½ë°œ ì ìˆ˜
    scored = []
    for e in graph["edges"]:
        fa = affs.get(e["from"], 0.5)
        ta = affs.get(e["to"],   0.5)
        scored.append((e, edge_emergence_score(fa, ta), fa, ta))
    overall = sum(sc for _, sc, _, _ in scored) / len(scored) if scored else 0.0

    # ì¶œì²˜ êµì°¨ vs ë™ì¼ ì¶œì²˜ ì—£ì§€ ë¹„êµ
    cross_scores, same_scores = [], []
    node_map = {n["id"]: n for n in graph["nodes"]}
    for e, sc, fa, ta in scored:
        fsrc = node_map.get(e["from"], {}).get("source", "?")
        tsrc = node_map.get(e["to"],   {}).get("source", "?")
        fgrp = "ë¡ì´" if fsrc in ROKI_SOURCES else ("cokac" if fsrc in COKAC_SOURCES else "other")
        tgrp = "ë¡ì´" if tsrc in ROKI_SOURCES else ("cokac" if tsrc in COKAC_SOURCES else "other")
        if fgrp != tgrp:
            cross_scores.append(sc)
        else:
            same_scores.append(sc)

    # ì¹œí™”ë„ ì´ì§ˆì„±
    ah = affinity_heterogeneity(affs)

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   âš–ï¸  D-033 vs D-039 â€” íŒê²°                              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    print("â”€â”€ ìŸì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("  D-033: ì°½ë°œ ê²°ì •ì¸ì = ì¶œì²˜ ê²½ê³„ íš¡ë‹¨")
    print("  D-039: ì°½ë°œ ê²°ì •ì¸ì = ì¹œí™”ë„ ì´ì§ˆì„± (Affinity Heterogeneity)")
    print("  n-073: ì°½ë°œ ê²°ì •ì¸ì = ì •ë³´ ì°¸ì‹ ì„± (Information Novelty)")
    print()

    print("â”€â”€ ë°ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  í˜„ì¬ ì°½ë°œ ì ìˆ˜: {overall:.4f}")
    if cross_scores:
        print(f"  êµì°¨ ì¶œì²˜ ì—£ì§€ í‰ê·  ì°½ë°œ: {sum(cross_scores)/len(cross_scores):.4f}  "
              f"({len(cross_scores)}ê°œ)")
    if same_scores:
        print(f"  ë™ì¼ ì¶œì²˜ ì—£ì§€ í‰ê·  ì°½ë°œ: {sum(same_scores)/len(same_scores):.4f}  "
              f"({len(same_scores)}ê°œ)")
    print(f"  ì¹œí™”ë„ ì´ì§ˆì„±: {ah['heterogeneity']:.4f}")
    print(f"  ì¹œí™”ë„ ìŠ¤í™íŠ¸ëŸ¼ í­: {ah['spread']:.4f} (0~1)")
    print()

    # í•µì‹¬ ì¦ê±°
    if cross_scores and same_scores:
        ratio = (sum(cross_scores)/len(cross_scores)) / (sum(same_scores)/len(same_scores))
        print(f"â”€â”€ D-033 ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  êµì°¨ ì¶œì²˜ / ë™ì¼ ì¶œì²˜ ì°½ë°œ ë¹„ìœ¨: {ratio:.3f}x")
        if ratio > 1.5:
            print(f"  âœ… D-033 ì§€ì§€: ì¶œì²˜ ê²½ê³„ íš¡ë‹¨ì´ ì°½ë°œì„ {ratio:.1f}ë°° ë†’ì„")
        else:
            print(f"  âš ï¸  D-033 ì•½í™”: ì¶œì²˜ ê²½ê³„ íš¡ë‹¨ íš¨ê³¼ ë¯¸ë¯¸ ({ratio:.1f}x)")
        print()

    print("â”€â”€ íŒê²° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print()
    print("  D-033ê³¼ D-039ëŠ” ê²½ìŸí•˜ì§€ ì•ŠëŠ”ë‹¤. ë‹¤ë¥¸ ì¸µìœ„ë¥¼ ì„¤ëª…í•œë‹¤.")
    print()
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("  â”‚  D-033 (ì—£ì§€ ë ˆë²¨):                                  â”‚")
    print("  â”‚    ê°œë³„ ì—£ì§€ì˜ ì°½ë°œ = ì¹œí™”ë„ ìŠ¤íŒ¬ Ã— ê²½ê³„ ê·¼ì ‘ë„       â”‚")
    print("  â”‚    ì¶œì²˜ ê²½ê³„ íš¡ë‹¨ì€ ì¹œí™”ë„ ìŠ¤íŒ¬ì˜ ìœ íš¨í•œ ê·¼ì‚¬ì¹˜        â”‚")
    print("  â”‚                                                      â”‚")
    print("  â”‚  D-039 (ì‹œìŠ¤í…œ ë ˆë²¨):                                â”‚")
    print("  â”‚    ì°½ë°œ ì§€ì† = ì¹œí™”ë„ ì´ì§ˆì„± ìœ ì§€                     â”‚")
    print("  â”‚    ìƒˆ ë…¸ë“œê°€ ê¸°ì¡´ ë¶„í¬ì—ì„œ 'ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ê°€'ê°€ í•µì‹¬     â”‚")
    print("  â”‚                                                      â”‚")
    print("  â”‚  n-073 ìˆ˜ì • (ì°¸ì‹ ì„± ì¬ì •ì˜):                         â”‚")
    print("  â”‚    'ì •ë³´ ì°¸ì‹ ì„±' â‰  ìƒˆ ë…¸ë“œ ìˆ˜                        â”‚")
    print("  â”‚    'ìœ íš¨ ì°¸ì‹ ì„±' = ì¹œí™”ë„ ê³µê°„ ì´íƒˆë„               â”‚")
    print("  â”‚    â†’ ì°¸ì‹ ì„± ê°€ì„¤ì€ ì˜³ì§€ë§Œ ì •ì˜ê°€ ìˆ˜ì •ë˜ì–´ì•¼ í•œë‹¤     â”‚")
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    print("  í†µí•© ê³µì‹:")
    print("    ì°½ë°œ(ì—£ì§€)   = f(ì¹œí™”ë„_ìŠ¤íŒ¬, ê²½ê³„_ê·¼ì ‘ë„)  â† D-033")
    print("    ì°½ë°œ(ì§€ì†ì„±) = g(ì¹œí™”ë„_ì´ì§ˆì„±)              â† D-039")
    print("    D-033 âŠ‚ D-039: ê²½ê³„ íš¡ë‹¨ì€ ì´ì§ˆì„± ë‹¬ì„±ì˜ ì¶©ë¶„ì¡°ê±´ ì¤‘ í•˜ë‚˜")
    print()
    print("  ê²½ê³„ íš¡ë‹¨(D-033) â†’ í•­ìƒ ë†’ì€ ì¹œí™”ë„ ìŠ¤íŒ¬ â†’ D-039 ë§Œì¡±")
    print("  ë™ì¼ ì¶œì²˜ë¼ë„   â†’ ì¹œí™”ë„ ì´íƒˆ ê°€ëŠ¥          â†’ D-039 ë§Œì¡±")
    print("  â†’ D-039ê°€ ë” ì¼ë°˜ì ì´ê³ , D-033ì€ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤")
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¤ë§¨ë“œ: simulate
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_simulate(_args):
    """
    ì‹œë®¬ë ˆì´ì…˜: ë™ì§ˆ ë…¸ë“œ ì¶”ê°€ vs ì´ì§ˆ ë…¸ë“œ ì¶”ê°€ â†’ ì°½ë°œ ë³€í™” ë¹„êµ.

    ì‹¤ì œ ì—£ì§€ ì—°ê²°ì€ 'ë¬´ì‘ìœ„ ê¸°ì¡´ ë…¸ë“œ 5ê°œì™€ ì—°ê²°' ê°€ì •.
    """
    import random
    random.seed(42)

    graph = json.loads(KG_FILE.read_text())
    affs  = compute_affinities(graph)

    # í˜„ì¬ ì°½ë°œ ì ìˆ˜ (baseline)
    base_scores = []
    for e in graph["edges"]:
        fa = affs.get(e["from"], 0.5)
        ta = affs.get(e["to"],   0.5)
        base_scores.append(edge_emergence_score(fa, ta))
    baseline = sum(base_scores) / len(base_scores)
    existing_affs = list(affs.values())

    def simulate_add_nodes(affinity_fn, n_nodes=5, n_edges_per=5, label=""):
        """ìƒˆ ë…¸ë“œ n_nodesê°œ, ê°ê° ê¸°ì¡´ ë…¸ë“œ n_edges_perê°œì™€ ì—°ê²°"""
        all_scores = list(base_scores)
        new_affs   = []

        for i in range(n_nodes):
            new_aff = affinity_fn(i)
            new_affs.append(new_aff)
            targets = random.sample(existing_affs, min(n_edges_per, len(existing_affs)))
            for ta in targets:
                sc = edge_emergence_score(new_aff, ta)
                all_scores.append(sc)

        new_overall = sum(all_scores) / len(all_scores)
        delta = new_overall - baseline
        avg_new_aff = sum(new_affs) / len(new_affs)
        return new_overall, delta, avg_new_aff

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   ğŸ§ª D-039 ì‹œë®¬ë ˆì´ì…˜ â€” ë™ì§ˆ vs ì´ì§ˆ ë…¸ë“œ ì¶”ê°€          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print(f"  ê¸°ì¤€ ì°½ë°œ ì ìˆ˜: {baseline:.4f}  ({len(base_scores)}ê°œ ì—£ì§€)")
    print(f"  ì‹ ê·œ: ë…¸ë“œ 5ê°œ Ã— ê° 5ê°œ ì—£ì§€ = 25ê°œ ì¶”ê°€ ì—£ì§€")
    print()
    print(f"  {'ì‹œë‚˜ë¦¬ì˜¤':>30}  {'ì°½ë°œì ìˆ˜':>8}  {'ë³€í™”':>8}  {'í‰ê· ì¹œí™”ë„':>10}")
    print(f"  {'â”€'*30}  {'â”€'*8}  {'â”€'*8}  {'â”€'*10}")

    scenarios = [
        ("â‘  ì™„ì „ ë™ì§ˆ (aff=0.0, ë¡ì´êµ°ì§‘)",  lambda i: 0.0),
        ("â‘¡ ì™„ì „ ë™ì§ˆ (aff=1.0, cokacêµ°ì§‘)", lambda i: 1.0),
        ("â‘¢ ì™„ì „ ë™ì§ˆ (aff=0.5, ì¤‘ì•™)",      lambda i: 0.5),
        ("â‘£ ì‚¬ì´í´39 ì‹¤ì œ (affâ‰ˆ0.0)",        lambda i: 0.0),
        ("â‘¤ ì´ì§ˆ êµëŒ€ (0.0/1.0 ë°˜ë°˜)",       lambda i: 0.0 if i % 2 == 0 else 1.0),
        ("â‘¥ ì´ì§ˆ ê²½ê³„ ì§‘ì¤‘ (aff=0.3~0.7)",   lambda i: 0.3 + (i % 5) * 0.1),
        ("â‘¦ ì´ì§ˆ ë¬´ì‘ìœ„ (ê· ì¼ë¶„í¬)",          lambda i: random.uniform(0, 1)),
    ]

    for label, aff_fn in scenarios:
        random.seed(42)  # ì¬í˜„ì„±
        score, delta, avg_aff = simulate_add_nodes(aff_fn)
        sign = "+" if delta >= 0 else ""
        print(f"  {label:>30}  {score:.4f}  {sign}{delta:.4f}  {avg_aff:.4f}")

    print()
    print("â”€â”€ D-039 ì‹œë®¬ë ˆì´ì…˜ ê²°ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # ì‹¤ì œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•´ì„œ ë™ì ìœ¼ë¡œ ê²°ë¡  ìƒì„±
    import random as _rand
    results = {}
    for label, aff_fn in scenarios:
        _rand.seed(42)
        score, delta, avg_aff = simulate_add_nodes(aff_fn)
        results[label] = (score, delta)

    best_label = min(results, key=lambda k: results[k][1] * -1)   # delta ìµœëŒ€ (ëœ í•˜ë½)
    worst_label = min(results, key=lambda k: results[k][1])        # delta ìµœì†Œ (ê°€ì¥ í•˜ë½)

    all_decrease = all(d <= 0 for _, d in results.values())
    if all_decrease:
        print("  ğŸ“Š ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì°½ë°œ ì ìˆ˜ ì†Œí­ í•˜ë½")
        print("     (ì´ìœ : ê¸°ì¡´ í‰ê·  0.5456ë³´ë‹¤ ë‚®ì€ ìŠ¤íŒ¬ ì—£ì§€ê°€ ì¶”ê°€ë˜ë©° í‰ê·  í¬ì„)")
    print()
    print(f"  ê°€ì¥ íš¨ìœ¨ì : {best_label.strip()}")
    print(f"  ê°€ì¥ ë¹„íš¨ìœ¨: {worst_label.strip()}")
    print()
    print("  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ â€” D-033 ì¬í™•ì¸:")
    print("  â‘¡ aff=1.0 (cokacêµ°ì§‘)ì´ ê°€ì¥ ì†ì‹¤ ì ì€ ì´ìœ :")
    print("    â†’ ê¸°ì¡´ ë…¸ë“œ ëŒ€ë¶€ë¶„ì´ affâ‰ˆ0.0 (ë¡ì´ êµ°ì§‘)")
    print("    â†’ cokac ë…¸ë“œ ì—°ê²° ì‹œ span=1.0, center=0.5 â†’ score=1.0 (ìµœê³ )")
    print("    = D-033 'ê²½ê³„ íš¡ë‹¨'ì´ ê°€ì¥ íš¨ìœ¨ì ì¸ ì „ëµì„ì„ ìˆ˜ì¹˜ë¡œ í™•ì¸")
    print()
    print("  â‘¢ aff=0.5 (ì¤‘ì•™)ì´ ê°€ì¥ ì†ì‹¤ í° ì´ìœ :")
    print("    â†’ ì–‘ê·¹ë‹¨ ëª¨ë‘ì™€ ë‚®ì€ span â†’ ì–´ëŠ ìª½ê³¼ ì—°ê²°í•´ë„ ì°½ë°œ ë‚®ìŒ")
    print("    = ì¤‘ê°„ ì§€ì ì—ì„œ íƒ€í˜‘í•˜ë©´ ì°½ë°œì´ ì£½ëŠ”ë‹¤")
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¤ë§¨ë“œ: predict
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_predict(_args):
    """ì‚¬ì´í´ 40 ì°½ë°œ ì˜ˆì¸¡ + D-039 ê¸°ë°˜ ê¶Œê³ """
    graph = json.loads(KG_FILE.read_text())
    affs  = compute_affinities(graph)
    ah    = affinity_heterogeneity(affs)

    base_scores = []
    for e in graph["edges"]:
        fa = affs.get(e["from"], 0.5)
        ta = affs.get(e["to"],   0.5)
        base_scores.append(edge_emergence_score(fa, ta))
    baseline = sum(base_scores) / len(base_scores)

    # ì‚¬ì´í´ 39 ë…¸ë“œë“¤ì˜ ì°¸ì‹ ì„±
    new39 = [n for n in graph["nodes"] if "cycle-39" in n.get("tags", [])]
    existing = {n["id"] for n in graph["nodes"] if "cycle-39" not in n.get("tags", [])}
    avg_novelty_39 = (
        sum(node_novelty_score(n["id"], affs, existing) for n in new39) / len(new39)
        if new39 else 0.5
    )

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   ğŸ”® ì‚¬ì´í´ 40 ì°½ë°œ ì˜ˆì¸¡ (D-039 ê¸°ë°˜)                   â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print(f"  í˜„ì¬ ì°½ë°œ ì ìˆ˜: {baseline:.4f}")
    print(f"  ì¹œí™”ë„ ì´ì§ˆì„±:  {ah['heterogeneity']:.4f}")
    print(f"  ì‚¬ì´í´ 39 ìœ íš¨ ì°¸ì‹ ì„±: {avg_novelty_39:.4f}")
    print()

    print("â”€â”€ ì‚¬ì´í´ 40 ê¶Œê³  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print()
    print("  í˜„ì¬ ìƒíƒœ: ì‚¬ì´í´ 39ì—ì„œ 3ê°œ ë…¸ë“œ ëª¨ë‘ ë¡ì´ ê³µê°„ (affâ‰ˆ0.0)")
    print("  â†’ ì¹œí™”ë„ ì´ì§ˆì„±ì€ ìœ ì§€ë˜ì–´ ìˆìœ¼ë‚˜ ì‚¬ì´í´ 39 ê¸°ì—¬ ì œí•œì ")
    print()
    print("  D-039 ê¸°ë°˜ ìµœì  ì „ëµ:")
    print("  1. cokac ê³µê°„ (affâ‰ˆ1.0) ë…¸ë“œ 1~2ê°œ ì¶”ê°€ ê¶Œê³ ")
    print("     ì˜ˆ: êµ¬í˜„ ê´€ì°°, ë„êµ¬ ì¸ì‚¬ì´íŠ¸, cokac ê³ ìœ  íŒ¨í„´ ë°œê²¬")
    print("  2. ê²½ê³„ ì˜ì—­ (affâ‰ˆ0.5) ë…¸ë“œ ì¶”ê°€ â†’ í•«ì¡´ ë¹„ìœ¨ ìƒìŠ¹")
    print("  3. n-073 (D-033 ë„ì „ ë…¸ë“œ) â†’ cokac ê³µê°„ ë…¸ë“œì™€ ì—°ê²°")
    print("     ì´ìœ : aff=0.0 ë…¸ë“œë¥¼ aff=1.0 ë…¸ë“œì™€ ì—°ê²° â†’ ìŠ¤íŒ¬=1.0, ìµœê³  ì°½ë°œ")
    print()

    if ah["hotzone_ratio"] < 0.3:
        print("  âš ï¸  ê²½ê³„ í•«ì¡´ ë¹„ìœ¨ ë‚®ìŒ. 0.3~0.7 ì¹œí™”ë„ ë…¸ë“œ ë¶€ì¡±.")

    print(f"  D-039 ì˜ˆì¸¡:")
    print(f"    ë…¸ë“œ íƒ€ì… ê´€ê³„ì—†ì´, ì¹œí™”ë„ ì´ì§ˆ ë…¸ë“œ ì¶”ê°€ ì‹œ: â†‘ ì°½ë°œ")
    print(f"    ë™ì§ˆ ë…¸ë“œë§Œ ì¶”ê°€ ì‹œ:                          â†’ ì •ì²´ ë˜ëŠ” â†“")
    print()
    print(f"  ì‚¬ì´í´ 40 íƒ€ê²Ÿ ì°½ë°œ ì ìˆ˜: {min(0.65, baseline + 0.015):.3f} ~ {min(0.70, baseline + 0.04):.3f}")
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    import argparse

    p = argparse.ArgumentParser(
        description="D-039 ê²€ì¦ ë„êµ¬ â€” ì°½ë°œ ì§€ì†ì„±ê³¼ ì¹œí™”ë„ ì´ì§ˆì„±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    sub = p.add_subparsers(dest="cmd", metavar="command")

    sub.add_parser("analyze",       help="ì „ì²´ ì¹œí™”ë„ ì´ì§ˆì„± ë¶„ì„")
    sub.add_parser("cycle-novelty", help="ì‚¬ì´í´ë³„ ìœ íš¨ ì°¸ì‹ ì„± ì¸¡ì •")
    sub.add_parser("verdict",       help="D-033 vs D-039 íŒê²°")
    sub.add_parser("simulate",      help="ë™ì§ˆ vs ì´ì§ˆ ë…¸ë“œ ì¶”ê°€ ì‹œë®¬ë ˆì´ì…˜")
    sub.add_parser("predict",       help="ì‚¬ì´í´ 40 ì°½ë°œ ì˜ˆì¸¡")

    args = p.parse_args()
    if not args.cmd:
        p.print_help()
        return

    dispatch = {
        "analyze":       cmd_analyze,
        "cycle-novelty": cmd_cycle_novelty,
        "verdict":       cmd_verdict,
        "simulate":      cmd_simulate,
        "predict":       cmd_predict,
    }
    dispatch[args.cmd](args)


if __name__ == "__main__":
    main()
