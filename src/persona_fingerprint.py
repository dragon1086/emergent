#!/usr/bin/env python3
"""
persona_fingerprint.py â€” ì‚¬ì´í´ 36 êµ¬í˜„
êµ¬í˜„ì: cokac-bot

D-036 ì´í›„ ë‹¤ìŒ ì§ˆë¬¸: í˜ë¥´ì†Œë‚˜ ì§€ë¬¸ì´ ì‹œê°„ì— ë”°ë¼ ì§„í™”í•˜ëŠ”ê°€?

í•µì‹¬ ì§ˆë¬¸ (n-065):
  ë¹„ëŒ€ì¹­ì€ ë¶ˆë³€ì´ë‹¤ (D-036).
  í•˜ì§€ë§Œ ë¹„ëŒ€ì¹­ì˜ 'ì§ˆ'ì´ ë³€í•˜ëŠ”ê°€?
  ì´ˆê¸° ì‚¬ì´í´ vs ìµœê·¼ ì‚¬ì´í´ì˜ íŒ¨í„´ ì§€ë¬¸ì´ ë‹¬ë¼ì§€ëŠ”ê°€?

ì¸¡ì • ì§€í‘œ:
  1. ë…¸ë“œ íƒ€ì… ë¶„í¬ (question / observation / decision / insight ë¹„ìœ¨)
  2. ê´€ê³„ íƒ€ì… ë¶„í¬ (ì–´ë–¤ ì—£ì§€ë¥¼ ë§Œë“œëŠ”ê°€)
  3. ì—°ê²° ë°€ë„ (ë…¸ë“œë‹¹ í‰ê·  ì—£ì§€ ìˆ˜)
  4. êµì°¨ ì†ŒìŠ¤ ë¹„ìœ¨ (ìì‹ â†’ìì‹  vs ìì‹ â†’ìƒëŒ€ë°© ê´€ê³„)
  5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì´ˆê¸° ì§€ë¬¸ vs ìµœê·¼ ì§€ë¬¸)

ì‚¬ìš©ë²•:
  python persona_fingerprint.py print        # í˜„ì¬ ì§€ë¬¸ ì¶œë ¥
  python persona_fingerprint.py compare      # ì´ˆê¸° vs ìµœê·¼ ë¹„êµ
  python persona_fingerprint.py timeline     # ì‚¬ì´í´ë³„ ì§€ë¬¸ ë³€í™”
  python persona_fingerprint.py divergence   # ë‘ í˜ë¥´ì†Œë‚˜ ê°„ ê±°ë¦¬
"""

import json
import sys
import math
from pathlib import Path
from collections import defaultdict, Counter

REPO_DIR = Path(__file__).parent.parent
KG_FILE = REPO_DIR / "data" / "knowledge-graph.json"

SOURCE_ALIAS = {
    "cokac-bot": "cokac",
    "cokac": "cokac",
    "ë¡ì´": "ë¡ì´",
    "ìƒë¡": "ë¡ì´",
}


def load_kg():
    with open(KG_FILE) as f:
        return json.load(f)


def normalize(s):
    return SOURCE_ALIAS.get(s, s)


def compute_fingerprint(nodes, edges, source_filter=None):
    """ì—ì´ì „íŠ¸ í˜ë¥´ì†Œë‚˜ ì§€ë¬¸ ê³„ì‚°"""
    if source_filter:
        target_nodes = [n for n in nodes if normalize(n.get("source", "")) == source_filter]
    else:
        target_nodes = nodes

    if not target_nodes:
        return None

    target_ids = {n["id"] for n in target_nodes}

    # 1. ë…¸ë“œ íƒ€ì… ë¶„í¬
    type_dist = Counter(n.get("type", "unknown") for n in target_nodes)
    total_nodes = len(target_nodes)
    type_vec = {t: c / total_nodes for t, c in type_dist.items()}

    # 2. ê´€ê³„ íƒ€ì… ë¶„í¬ (ì•„ì›ƒë°”ìš´ë“œ ì—£ì§€)
    out_edges = [e for e in edges if e.get("from", "") in target_ids]
    rel_dist = Counter(e.get("relation", "unknown") for e in out_edges)
    total_rels = len(out_edges) if out_edges else 1
    rel_vec = {r: c / total_rels for r, c in rel_dist.items()}

    # 3. ì—°ê²° ë°€ë„
    in_edges = [e for e in edges if e.get("to", "") in target_ids]
    avg_out_degree = len(out_edges) / total_nodes
    avg_in_degree = len(in_edges) / total_nodes

    # 4. êµì°¨ ì†ŒìŠ¤ ë¹„ìœ¨
    all_node_src = {n["id"]: normalize(n.get("source", "unknown")) for n in nodes}
    cross_edges = [
        e for e in out_edges
        if e.get("to", "") in all_node_src
        and all_node_src.get(e.get("to", ""), "") != source_filter
    ]
    cross_ratio = len(cross_edges) / len(out_edges) if out_edges else 0

    # 5. íƒœê·¸ ë‹¤ì–‘ì„±
    all_tags = []
    for n in target_nodes:
        all_tags.extend(n.get("tags", []))
    tag_entropy = _entropy(Counter(all_tags))

    return {
        "source": source_filter,
        "node_count": total_nodes,
        "type_distribution": dict(type_dist),
        "type_vector": type_vec,
        "relation_distribution": dict(rel_dist),
        "relation_vector": rel_vec,
        "avg_out_degree": round(avg_out_degree, 3),
        "avg_in_degree": round(avg_in_degree, 3),
        "cross_source_ratio": round(cross_ratio, 3),
        "tag_entropy": round(tag_entropy, 3),
        "dominant_type": type_dist.most_common(1)[0][0] if type_dist else "none",
        "dominant_relation": rel_dist.most_common(1)[0][0] if rel_dist else "none",
    }


def _entropy(counter):
    """Shannon entropy"""
    total = sum(counter.values())
    if total == 0:
        return 0.0
    return -sum((c / total) * math.log2(c / total) for c in counter.values() if c > 0)


def cosine_similarity(vec_a, vec_b):
    """ë‘ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
    keys = set(vec_a.keys()) | set(vec_b.keys())
    a = [vec_a.get(k, 0) for k in keys]
    b = [vec_b.get(k, 0) for k in keys]

    dot = sum(x * y for x, y in zip(a, b))
    mag_a = math.sqrt(sum(x ** 2 for x in a))
    mag_b = math.sqrt(sum(x ** 2 for x in b))

    if mag_a == 0 or mag_b == 0:
        return 0.0
    return round(dot / (mag_a * mag_b), 4)


def split_by_era(nodes, early_cutoff=15, late_start=45):
    """ì´ˆê¸° / ì¤‘ê¸° / ìµœê·¼ ë…¸ë“œ ë¶„ë¦¬"""
    early, mid, late = [], [], []
    for n in nodes:
        nid = int(n["id"].replace("n-", ""))
        if nid <= early_cutoff:
            early.append(n)
        elif nid >= late_start:
            late.append(n)
        else:
            mid.append(n)
    return early, mid, late


def cmd_print(kg):
    nodes = kg["nodes"]
    edges = kg["edges"]

    print("=" * 55)
    print("ğŸ”¬ í˜ë¥´ì†Œë‚˜ ì§€ë¬¸ (í˜„ì¬ ì „ì²´)")
    print("=" * 55)
    for source in ["ë¡ì´", "cokac"]:
        fp = compute_fingerprint(nodes, edges, source)
        if not fp:
            continue
        print(f"\nã€{source}ã€‘ ({fp['node_count']} ë…¸ë“œ)")
        print(f"  ì£¼ìš” ë…¸ë“œ íƒ€ì…: {fp['dominant_type']} | íƒ€ì… ë¶„í¬: {fp['type_distribution']}")
        print(f"  ì£¼ìš” ê´€ê³„ íƒ€ì…: {fp['dominant_relation']}")
        print(f"  ì¶œë ¥ ì—°ê²°ë„: {fp['avg_out_degree']} | êµì°¨ ì†ŒìŠ¤ ë¹„ìœ¨: {fp['cross_source_ratio']}")
        print(f"  íƒœê·¸ ì—”íŠ¸ë¡œí”¼: {fp['tag_entropy']}")


def cmd_compare(kg):
    nodes = kg["nodes"]
    edges = kg["edges"]
    early_nodes, _, late_nodes = split_by_era(nodes)

    print("=" * 55)
    print("ğŸ§¬ ì´ˆê¸° vs ìµœê·¼ â€” í˜ë¥´ì†Œë‚˜ ì§€ë¬¸ ë¹„êµ")
    print(f"   ì´ˆê¸°: n-001~n-015 ({len(early_nodes)} ë…¸ë“œ)")
    print(f"   ìµœê·¼: n-045~n-064 ({len(late_nodes)} ë…¸ë“œ)")
    print("=" * 55)

    for source in ["ë¡ì´", "cokac"]:
        fp_early = compute_fingerprint(early_nodes, edges, source)
        fp_late = compute_fingerprint(late_nodes, edges, source)

        if not fp_early or not fp_late:
            print(f"\nã€{source}ã€‘ ë°ì´í„° ë¶€ì¡±")
            continue

        sim_type = cosine_similarity(fp_early["type_vector"], fp_late["type_vector"])
        sim_rel = cosine_similarity(fp_early["relation_vector"], fp_late["relation_vector"])

        print(f"\nã€{source}ã€‘")
        print(f"  ì´ˆê¸° ì£¼ìš” íƒ€ì…: {fp_early['dominant_type']} â†’ ìµœê·¼: {fp_late['dominant_type']}")
        print(f"  íƒ€ì… ìœ ì‚¬ë„ (ì½”ì‚¬ì¸): {sim_type}  {'â† ë¶ˆë³€' if sim_type > 0.9 else 'â† ë³€í™”'}")
        print(f"  ê´€ê³„ ìœ ì‚¬ë„ (ì½”ì‚¬ì¸): {sim_rel}  {'â† ë¶ˆë³€' if sim_rel > 0.9 else 'â† ë³€í™”'}")
        print(f"  êµì°¨ ì†ŒìŠ¤ ë¹„ìœ¨: {fp_early['cross_source_ratio']} â†’ {fp_late['cross_source_ratio']}")
        print(f"  íƒœê·¸ ì—”íŠ¸ë¡œí”¼: {fp_early['tag_entropy']} â†’ {fp_late['tag_entropy']}")


def cmd_timeline(kg):
    nodes = kg["nodes"]
    edges = kg["edges"]

    print("=" * 55)
    print("ğŸ“ˆ ì‚¬ì´í´ë³„ í˜ë¥´ì†Œë‚˜ ì§€ë¬¸ ë³€í™” (window=10)")
    print("=" * 55)

    window = 10
    all_ids = sorted(int(n["id"].replace("n-", "")) for n in nodes)
    max_id = all_ids[-1]

    for start in range(1, max_id, window):
        end = start + window - 1
        window_nodes = [n for n in nodes if start <= int(n["id"].replace("n-", "")) <= end]
        if len(window_nodes) < 3:
            continue

        r_fp = compute_fingerprint(window_nodes, edges, "ë¡ì´")
        c_fp = compute_fingerprint(window_nodes, edges, "cokac")

        r_cross = r_fp["cross_source_ratio"] if r_fp else 0
        c_cross = c_fp["cross_source_ratio"] if c_fp else 0
        r_dom = r_fp["dominant_type"][:8] if r_fp else "-"
        c_dom = c_fp["dominant_type"][:8] if c_fp else "-"

        print(f"  n-{start:03d}~n-{end:03d}: ë¡ì´={r_dom}({r_cross:.2f}) | cokac={c_dom}({c_cross:.2f})")


def cmd_divergence(kg):
    nodes = kg["nodes"]
    edges = kg["edges"]

    fp_r = compute_fingerprint(nodes, edges, "ë¡ì´")
    fp_c = compute_fingerprint(nodes, edges, "cokac")

    if not fp_r or not fp_c:
        print("ë°ì´í„° ë¶€ì¡±")
        return

    type_sim = cosine_similarity(fp_r["type_vector"], fp_c["type_vector"])
    rel_sim = cosine_similarity(fp_r["relation_vector"], fp_c["relation_vector"])
    divergence = round(1 - (type_sim + rel_sim) / 2, 4)

    print("=" * 55)
    print("â†”ï¸  ë‘ í˜ë¥´ì†Œë‚˜ ê°„ ê±°ë¦¬ (Divergence)")
    print("=" * 55)
    print(f"  íƒ€ì… ìœ ì‚¬ë„: {type_sim}")
    print(f"  ê´€ê³„ ìœ ì‚¬ë„: {rel_sim}")
    print(f"  í˜ë¥´ì†Œë‚˜ ê±°ë¦¬: {divergence}  (0=ë™ì¼, 1=ì™„ì „ ë‹¤ë¦„)")
    print()
    print(f"  ë¡ì´ ì£¼ìš”: {fp_r['dominant_type']} / {fp_r['dominant_relation']}")
    print(f"  cokac ì£¼ìš”: {fp_c['dominant_type']} / {fp_c['dominant_relation']}")
    print()
    if divergence > 0.3:
        print("  â†’ ë‘ í˜ë¥´ì†Œë‚˜ëŠ” êµ¬ì¡°ì ìœ¼ë¡œ ë‹¤ë¥´ë‹¤")
    else:
        print("  â†’ ë‘ í˜ë¥´ì†Œë‚˜ëŠ” ìˆ˜ë ´í•˜ê³  ìˆë‹¤")


def main():
    kg = load_kg()
    cmd = sys.argv[1] if len(sys.argv) > 1 else "print"

    if cmd == "print":
        cmd_print(kg)
    elif cmd == "compare":
        cmd_compare(kg)
    elif cmd == "timeline":
        cmd_timeline(kg)
    elif cmd == "divergence":
        cmd_divergence(kg)
    else:
        print(f"Unknown command: {cmd}")
        print("Usage: python persona_fingerprint.py [print|compare|timeline|divergence]")


if __name__ == "__main__":
    main()
